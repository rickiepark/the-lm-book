{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rickiepark/the-lm-book/blob/main/emotion_GPT2_as_text_generator_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fl7Fu-B4uARb"
   },
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <div style=\"background-color: #f4f6f7; padding: 15px; width: 80%;\">\n",
    "        <table style=\"width: 100%\">\n",
    "            <tr>\n",
    "                <td style=\"vertical-align: middle;\">\n",
    "                    <span style=\"font-size: 14px;\">\n",
    "                        A notebook for <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">The Hundred-Page Language Models Book</a> by Andriy Burkov<br><br>\n",
    "                        Code repository: <a href=\"https://github.com/rickiepark/the-lm-book\" target=\"_blank\" rel=\"noopener\">https://github.com/rickiepark/the-lm-book</a>\n",
    "                    </span>\n",
    "                </td>\n",
    "                <td style=\"vertical-align: middle;\">\n",
    "                    <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">\n",
    "                        <img src=\"https://thelmbook.com/img/book.png\" width=\"80px\" alt=\"The Hundred-Page Language Models Book\">\n",
    "                    </a>\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yy0zjL_2ouOU"
   },
   "outputs": [],
   "source": [
    "# 필수 라이브러리 임포트\n",
    "import json            # JSON 데이터 구문 분석을 위한 라이브러리\n",
    "import random          # 시드 설정 및 데이터 셔플링을 위한 라이브러리\n",
    "import gzip            # 데이터셋 압축 해제를 위한 라이브러리\n",
    "import requests        # URL에서 데이터셋 다운로드를 위한 라이브러리\n",
    "import torch           # 메인 파이토치 라이브러리\n",
    "from peft import get_peft_model, LoraConfig, TaskType  # LoRA를 사용한 효율적인 미세 튜닝을 위한 라이브러리\n",
    "from torch.utils.data import Dataset, DataLoader  # 데이터셋 처리를 위한 라이브러리\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # 허깅 페이스 모델 구성 요소\n",
    "from torch.optim import AdamW    # 훈련을 위한 옵티마이저\n",
    "from tqdm import tqdm   # 진행률 표시줄 유틸리티\n",
    "import re               # 텍스트 정규화를 위한 라이브러리\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    다양한 라이브러리에서 재현성을 위해 난수 시드를 설정합니다.\n",
    "    Args:\n",
    "        seed (int): 난수 생성을 위한 시드 값\n",
    "    \"\"\"\n",
    "    # 파이썬 내장 random 시드 설정\n",
    "    random.seed(seed)\n",
    "    # 파이토치 CPU 난수 시드 설정\n",
    "    torch.manual_seed(seed)\n",
    "    # 사용 가능한 모든 GPU에 대한 시드 설정\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # cuDNN이 결정론적 알고리즘을 사용하도록 요청\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # 일관된 동작을 위해 cuDNN의 자동 튜너 비활성화\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def build_prompt(text):\n",
    "    \"\"\"\n",
    "    감정 분류를 위한 표준화된 프롬프트를 생성합니다.\n",
    "    Args:\n",
    "        text (str): 분류할 입력 텍스트\n",
    "    Returns:\n",
    "        str: 모델을 위한 형식화된 프롬프트\n",
    "    \"\"\"\n",
    "    # 입력 텍스트를 일관된 프롬프트 구조로 형식화\n",
    "    return f\"Predict the emotion for the following text: {text}\\nEmotion:\"\n",
    "\n",
    "def encode_text(tokenizer, text, return_tensor=False):\n",
    "    \"\"\"\n",
    "    제공된 토크나이저를 사용하여 텍스트를 인코딩합니다.\n",
    "    Args:\n",
    "        tokenizer: 허깅 페이스 토크나이저\n",
    "        text (str): 인코딩할 텍스트\n",
    "        return_tensor (bool): 파이토치 텐서를 반환할지 여부\n",
    "    Returns:\n",
    "        토큰 ID의 목록 또는 텐서\n",
    "    \"\"\"\n",
    "    # 텐서 출력이 요청된 경우, 파이토치 텐서로 인코딩\n",
    "    if return_tensor:\n",
    "        return tokenizer.encode(\n",
    "            text, add_special_tokens=False, return_tensors=\"pt\"\n",
    "        )\n",
    "    # 그렇지 않으면 토큰 ID 목록 반환\n",
    "    else:\n",
    "        return tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "def decode_text(tokenizer, token_ids):\n",
    "    \"\"\"\n",
    "    토큰 ID를 텍스트로 다시 디코딩합니다.\n",
    "    Args:\n",
    "        tokenizer: 허깅 페이스 토크나이저\n",
    "        token_ids: 토큰 ID의 목록 또는 텐서\n",
    "    Returns:\n",
    "        str: 디코딩된 텍스트\n",
    "    \"\"\"\n",
    "    # 특수 토큰을 건너뛰고 토큰 ID를 텍스트로 다시 변환\n",
    "    return tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "\n",
    "class PromptCompletionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    프롬프트-완성 쌍을 위한 파이토치 데이터셋.\n",
    "    텍스트 데이터를 모델 준비 형식으로 변환하는 작업을 처리합니다.\n",
    "    Args:\n",
    "        data (list): 프롬프트와 완성을 포함하는 사전 목록\n",
    "        tokenizer: 허깅 페이스 토크나이저\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer):\n",
    "        # 나중에 사용하기 위해 원시 데이터와 토크나이저 저장\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 총 샘플 수 반환\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        단일 훈련 샘플을 반환합니다.\n",
    "        Args:\n",
    "            idx (int): 가져올 샘플의 인덱스\n",
    "        Returns:\n",
    "            dict: input_ids, labels, prompt, expected_completion을 포함\n",
    "        \"\"\"\n",
    "        # 데이터셋에서 특정 샘플 가져오기\n",
    "        item = self.data[idx]\n",
    "        prompt = item[\"prompt\"]\n",
    "        completion = item[\"completion\"]\n",
    "        # 프롬프트와 완성 모두에 대해 텍스트를 토큰 ID로 변환\n",
    "        encoded_prompt = encode_text(self.tokenizer, prompt)\n",
    "        encoded_completion = encode_text(self.tokenizer, completion)\n",
    "        # 시퀀스 끝 토큰 ID 가져오기\n",
    "        eos_token = self.tokenizer.eos_token_id\n",
    "        # EOS 토큰과 함께 프롬프트와 완성 토큰 결합\n",
    "        input_ids = encoded_prompt + encoded_completion + [eos_token]\n",
    "        # 레이블 생성: 프롬프트의 경우 -100(손실에서 무시됨), 학습을 위한 완성 토큰\n",
    "        labels = [-100] * len(encoded_prompt) + encoded_completion + [eos_token]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"prompt\": prompt,\n",
    "            \"expected_completion\": completion\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    샘플 배치를 훈련 준비 형식으로 통합합니다.\n",
    "    패딩과 텐서 변환을 처리합니다.\n",
    "    Args:\n",
    "        batch: 데이터셋의 샘플 목록\n",
    "    Returns:\n",
    "        tuple: (input_ids, attention_mask, labels, prompts, expected_completions)\n",
    "    \"\"\"\n",
    "    # 패딩을 위해 배치에서 가장 긴 시퀀스 찾기\n",
    "    max_length = max(len(item[\"input_ids\"]) for item in batch)\n",
    "    # 패딩 토큰으로 입력 시퀀스를 max_length로 패딩\n",
    "    input_ids = [\n",
    "        item[\"input_ids\"] +\n",
    "        [tokenizer.pad_token_id] * (max_length - len(item[\"input_ids\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "    # -100으로 레이블 시퀀스 패딩(손실 계산에서 무시됨)\n",
    "    labels = [\n",
    "        item[\"labels\"] +\n",
    "        [-100] * (max_length - len(item[\"labels\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "    # 어텐션 마스크 생성: 실제 토큰은 1, 패딩은 0\n",
    "    attention_mask = [\n",
    "        [1] * len(item[\"input_ids\"]) +\n",
    "        [0] * (max_length - len(item[\"input_ids\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "    # 평가를 위해 원본 프롬프트와 완성 유지\n",
    "    prompts = [item[\"prompt\"] for item in batch]\n",
    "    expected_completions = [item[\"expected_completion\"] for item in batch]\n",
    "    # 텍스트를 제외한 모든 항목을 파이토치 텐서로 변환\n",
    "    return (\n",
    "        torch.tensor(input_ids),\n",
    "        torch.tensor(attention_mask),\n",
    "        torch.tensor(labels),\n",
    "        prompts,\n",
    "        expected_completions\n",
    "    )\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    일관된 비교를 위해 텍스트를 정규화합니다.\n",
    "    Args:\n",
    "        text (str): 입력 텍스트\n",
    "    Returns:\n",
    "        str: 정규화된 텍스트\n",
    "    \"\"\"\n",
    "    # 선행/후행 공백 제거 및 소문자로 변환\n",
    "    text = text.strip().lower()\n",
    "    # 여러 공백 문자를 단일 공백으로 대체\n",
    "    text = re.sub(r\"\\s+\", ' ', text)\n",
    "    return text\n",
    "\n",
    "def calculate_accuracy(model, tokenizer, loader):\n",
    "    \"\"\"\n",
    "    데이터셋에서 예측 정확도를 계산합니다.\n",
    "    Args:\n",
    "        model: 미세 튜닝된 모델\n",
    "        tokenizer: 연관된 토크나이저\n",
    "        loader: 평가 샘플을 포함하는 DataLoader\n",
    "    Returns:\n",
    "        float: 정확도 점수\n",
    "    \"\"\"\n",
    "    # 모델을 평가 모드로 설정\n",
    "    model.eval()\n",
    "    # 정확도 계산을 위한 카운터 초기화\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 효율성을 위해 그레이디언트 계산 비활성화\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels, prompts, expected_completions in loader:\n",
    "            for prompt, expected_completion in zip(prompts, expected_completions):\n",
    "                # 모델의 예측 생성\n",
    "                generated_text = generate_text(model, tokenizer, prompt)\n",
    "                # 예측과 타깃의 정규화된 버전 비교\n",
    "                if normalize_text(generated_text) == normalize_text(expected_completion):\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    # 정확도 계산, 빈 데이터셋 경우 처리\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    # 모델을 훈련 모드로 재설정\n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    주어진 프롬프트에 대한 텍스트 완성을 생성합니다.\n",
    "    Args:\n",
    "        model: 미세 튜닝된 모델\n",
    "        tokenizer: 연관된 토크나이저\n",
    "        prompt (str): 입력 프롬프트\n",
    "        max_new_tokens (int): 생성할 최대 토큰 수\n",
    "    Returns:\n",
    "        str: 생성된 완성\n",
    "    \"\"\"\n",
    "    # 프롬프트 인코딩 및 모델 장치로 이동\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    # 모델의 generate 메서드를 사용하여 완성 생성\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids[\"input_ids\"],\n",
    "        attention_mask=input_ids[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )[0]\n",
    "    # 생성된 부분만 추출 및 디코딩(프롬프트 제외)\n",
    "    generated_text = decode_text(tokenizer, output_ids[input_ids[\"input_ids\"].shape[1]:])\n",
    "    return generated_text.strip()\n",
    "\n",
    "def test_model(model_path, test_input):\n",
    "    \"\"\"\n",
    "    저장된 모델을 단일 입력으로 테스트합니다.\n",
    "    Args:\n",
    "        model_path (str): 저장된 모델의 경로\n",
    "        test_input (str): 분류할 텍스트\n",
    "    \"\"\"\n",
    "    # 장치 결정(사용 가능한 경우 GPU, 그렇지 않으면 CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"사용 중인 장치: {device}\")\n",
    "    # 저장된 모델과 토크나이저 로드\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    # 패딩 토큰 구성\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    # 예측 생성 및 표시\n",
    "    prompt = build_prompt(test_input)\n",
    "    generated_text = generate_text(model, tokenizer, prompt)\n",
    "    print(f\"입력: {test_input}\")\n",
    "    print(f\"생성된 감정: {generated_text}\")\n",
    "\n",
    "def download_and_prepare_data(data_url, tokenizer, batch_size, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    훈련을 위해 데이터셋을 다운로드하고 준비합니다.\n",
    "    Args:\n",
    "        data_url (str): 데이터셋의 URL\n",
    "        tokenizer: 텍스트 처리를 위한 토크나이저\n",
    "        batch_size (int): DataLoader의 배치 크기\n",
    "        test_ratio (float): 테스트를 위한 데이터 비율\n",
    "    Returns:\n",
    "        tuple: (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # 데이터셋 다운로드 및 압축 해제\n",
    "    response = requests.get(data_url)\n",
    "    content = gzip.decompress(response.content).decode()\n",
    "    # 각 예시를 프롬프트-완성 쌍으로 처리\n",
    "    dataset = []\n",
    "    for entry in map(json.loads, content.splitlines()):\n",
    "        dataset.append({\n",
    "            \"prompt\": build_prompt(entry['text']),\n",
    "            \"completion\": entry[\"label\"].strip()\n",
    "        })\n",
    "    # 훈련 및 테스트 세트로 분할\n",
    "    random.shuffle(dataset)\n",
    "    split_index = int(len(dataset) * (1 - test_ratio))\n",
    "    train_data = dataset[:split_index]\n",
    "    test_data = dataset[split_index:]\n",
    "    # 데이터셋 생성\n",
    "    train_dataset = PromptCompletionDataset(train_data, tokenizer)\n",
    "    test_dataset = PromptCompletionDataset(test_data, tokenizer)\n",
    "    # 데이터 로더 생성\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_hyperparameters():\n",
    "    \"\"\"\n",
    "    훈련 하이퍼파라미터를 반환합니다.\n",
    "    Returns:\n",
    "        tuple: (num_epochs, batch_size, learning_rate)\n",
    "    \"\"\"\n",
    "    # LoRA로 더 효율적이므로 더 많은 에포크로 훈련\n",
    "    num_epochs = 18\n",
    "    # 배치 크기\n",
    "    batch_size = 16\n",
    "    # 트랜스포머 미세 튜닝을 위한 표준 학습률\n",
    "    learning_rate = 5e-5\n",
    "    return num_epochs, batch_size, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ee3087f905bc4da78749da48ddb47e1f",
      "43aba598b6b94aaea38d2d17103ac304",
      "bd4847e184264672b3ce624fdea285ad",
      "4e86fe13a27441a69467f498d976b585",
      "5255d05023904c3f85b0a1a7df6699c4",
      "372ed7714aba4709a5b1b7973f23cf3c",
      "976e4145b48743d2880cd15b3544c0a8",
      "e2b8510c8b554bf2933a01987dd7f236",
      "9f45b2db8ee24c8c951fd1f02e7e95d5",
      "0af1f45b3ece44f4881ac6fbe7530a38",
      "ad320f1596ce4544a314228689479976",
      "0a6a590d1f504e7ca68785c04f41bc99",
      "278b1a9d034a4d7fa2adae13f63583c1",
      "6342945d4456479dadbb0e33a1b9fde9",
      "9e4526e63b384275aa2527dc4dcdfd35",
      "df82552984d24045ba265e264eab2699",
      "48f9bf7c36b34098919184fff0783e01",
      "af0b56fd97f5400fbfd811acbcc4dc68",
      "33b9f683c48d41ca96a92715c5b1c44b",
      "2694cb4442ef4827a6ebb01b8003c17f",
      "1e766903bd6343f1ae473b78be9c804a",
      "243d191c4679446f9dbe5e26a98ce1f8",
      "7f32a448d7584afd8ec6e49835ef39dd",
      "86c27e0045bf4423804f09d750f640d2",
      "0a3b58ff37724284b1152cfc0a7be9ed",
      "479f476f1d5f45789ae22bd253028d3c",
      "a2c5b2647be4413e9e2d6d017b14b75f",
      "b4732e963a704819b030f4bf5241e83b",
      "a92a74471da7400091938aa02eb3b29f",
      "fdcede06616b42c8927f75c8677789a7",
      "fa2c9dc27e704f56bb5dc6a505419984",
      "96338e8fe3d843ae89762ac529ff73e0",
      "19ecfc5fc6f840f880300abce32f156d",
      "e7c9046ae0d140fe8d3c18583a15cfa7",
      "5be08a35f6a045c694e838c7cb4c0dfe",
      "cde7f33825524e65a170a0c2c9e9aab1",
      "e22413bd22a1422fb214035edafeea94",
      "f5a3393385b44604b4e9407fcb9cc7bb",
      "749425c750794fca840933a51c08d894",
      "224a7abcf3ac4841b089dd3b9a855ef7",
      "2663524763e74a19a0c9e5b26ed3be2d",
      "9227aeb9419447f5855ee0fc110ec1aa",
      "f012c1d10d2e467ea9d301437c6e943a",
      "96fdab18595b4c91ae21587855b49607",
      "f7999ff30ed94392bd3df907152eebeb",
      "a4766bcdd8b94686806cc836fa0ec67d",
      "48c935d99de849e2b4db4093ac533c98",
      "2dfe5a47449246ccac823f0153e5781b",
      "1b5db668c3fe491099109e84282239a8",
      "f39cf23a85cd4aeea60f7d89d02f05f0",
      "8effbdbef8cd47d394d227868a610d52",
      "ae0409e641b4475fb523f57a314e1937",
      "c31aad57d6e44139a28f62a47a18ebf5",
      "7700f2eac0a94e5daf3dbbad653fe4a3",
      "7a34ba7ab301411096f7df831a5e4fb5",
      "ff2aa438348d4e8486a0aaa81d258e9d",
      "3ea854dec0df4de3879d52fe1c2571ed",
      "7a997fe9f8b54ff4992de0543e3068b5",
      "3bca881403e34c7aa05ead379cb5d845",
      "43ec2faeb8f64e0f9898fd48a572b4a1",
      "7933a478e45c49c0ac9b07ca317e7da0",
      "d8fb7aa6b1414117b7378ddaf3c54863",
      "f48cd542a6754ddeabc4468be6f1f40c",
      "4ce8e594ae9548abb443bc7613dfc501",
      "201be08c9f1a497182f3300684d3d916",
      "f2c1c3cc556742b8b0fd29c2526c3828",
      "9fa75473490a4a44a51c4b4ad5286fd8",
      "3934da3ce2d84f0d9c273a38fb583da4",
      "d1668a1789c0456fb70a1f14dd6ec7b0",
      "a000adac4a4642d6a8bd49a3b2df00dc",
      "719e13ab810949a39fd38db11ebb6c52",
      "c81113d72a7045b8adfd7ddac47f7051",
      "b8837cc98daf4516b9e10d5701a167ee",
      "4d0ee2201f5f4c4bb69965336e8f6019",
      "f597d21eb9c34f07bd0210f307cd19a8",
      "14384c8cfbda412c96c9b50384fb90e1",
      "dfb0a55bf566406aa6cccf742c5b6755"
     ]
    },
    "id": "VZaNhDZWgWMU",
    "outputId": "9bc94ca5-f3cb-4c49-ddd4-54bd50d79d76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 장치: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3087f905bc4da78749da48ddb47e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6a590d1f504e7ca68785c04f41bc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f32a448d7584afd8ec6e49835ef39dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c9046ae0d140fe8d3c18583a15cfa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7999ff30ed94392bd3df907152eebeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2aa438348d4e8486a0aaa81d258e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa75473490a4a44a51c4b4ad5286fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "에포크 1/18:   0%|          | 0/1125 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "에포크 1/18: 100%|██████████| 1125/1125 [00:57<00:00, 19.57it/s, 손실=0.613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 1 - 평균 손실: 0.6127, 테스트 정확도: 0.7605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 2/18: 100%|██████████| 1125/1125 [00:57<00:00, 19.69it/s, 손실=0.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 2 - 평균 손실: 0.3532, 테스트 정확도: 0.7970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 3/18: 100%|██████████| 1125/1125 [00:56<00:00, 19.77it/s, 손실=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 3 - 평균 손실: 0.2375, 테스트 정확도: 0.8530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 4/18: 100%|██████████| 1125/1125 [00:56<00:00, 19.75it/s, 손실=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 4 - 평균 손실: 0.1843, 테스트 정확도: 0.8985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 5/18: 100%|██████████| 1125/1125 [00:56<00:00, 19.88it/s, 손실=0.146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 5 - 평균 손실: 0.1457, 테스트 정확도: 0.9175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 6/18: 100%|██████████| 1125/1125 [00:56<00:00, 19.79it/s, 손실=0.121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 6 - 평균 손실: 0.1208, 테스트 정확도: 0.9215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 7/18: 100%|██████████| 1125/1125 [00:57<00:00, 19.65it/s, 손실=0.103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 7 - 평균 손실: 0.1028, 테스트 정확도: 0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 8/18: 100%|██████████| 1125/1125 [00:56<00:00, 19.80it/s, 손실=0.0927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 8 - 평균 손실: 0.0927, 테스트 정확도: 0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 9/18: 100%|██████████| 1125/1125 [00:56<00:00, 19.83it/s, 손실=0.0887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 9 - 평균 손실: 0.0887, 테스트 정확도: 0.9330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 10/18: 100%|██████████| 1125/1125 [00:56<00:00, 19.87it/s, 손실=0.079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 10 - 평균 손실: 0.0790, 테스트 정확도: 0.9315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 11/18: 100%|██████████| 1125/1125 [00:56<00:00, 20.05it/s, 손실=0.0771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 11 - 평균 손실: 0.0771, 테스트 정확도: 0.9325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 12/18: 100%|██████████| 1125/1125 [00:57<00:00, 19.72it/s, 손실=0.0699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 12 - 평균 손실: 0.0699, 테스트 정확도: 0.9345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 13/18: 100%|██████████| 1125/1125 [00:57<00:00, 19.64it/s, 손실=0.0663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 13 - 평균 손실: 0.0663, 테스트 정확도: 0.9265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 14/18: 100%|██████████| 1125/1125 [00:56<00:00, 19.89it/s, 손실=0.064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 14 - 평균 손실: 0.0640, 테스트 정확도: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 15/18: 100%|██████████| 1125/1125 [00:56<00:00, 19.88it/s, 손실=0.0633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 15 - 평균 손실: 0.0633, 테스트 정확도: 0.9380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 16/18: 100%|██████████| 1125/1125 [00:56<00:00, 19.80it/s, 손실=0.0605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 16 - 평균 손실: 0.0605, 테스트 정확도: 0.9390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 17/18: 100%|██████████| 1125/1125 [00:56<00:00, 20.02it/s, 손실=0.0571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 17 - 평균 손실: 0.0571, 테스트 정확도: 0.9370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 18/18: 100%|██████████| 1125/1125 [00:56<00:00, 19.95it/s, 손실=0.0574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 18 - 평균 손실: 0.0574, 테스트 정확도: 0.9420\n",
      "훈련 정확도: 0.9423\n",
      "테스트 정확도: 0.9420\n",
      "사용 중인 장치: cuda\n",
      "입력: I'm so happy to be able to finetune an LLM!\n",
      "생성된 감정: joy\n"
     ]
    }
   ],
   "source": [
    "# 재현성을 위해 난수 시드 설정\n",
    "set_seed(42)\n",
    "\n",
    "# 기본 훈련 매개변수 구성\n",
    "data_url = \"https://www.thelmbook.com/data/emotions\"\n",
    "model_name = \"openai-community/gpt2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 장치: {device}\")\n",
    "\n",
    "# 토크나이저 초기화\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# LoRA 매개변수 구성\n",
    "peft_config = LoraConfig(\n",
    "    task_type = TaskType.CAUSAL_LM,  # 코잘 언어 모델링을 위한 작업 유형 설정\n",
    "    inference_mode = False,          # 훈련 모드 활성화\n",
    "    r = 16,                          # LoRA 업데이트 행렬의 랭크\n",
    "    lora_alpha = 32                  # LoRA 스케일링 계수\n",
    ")\n",
    "\n",
    "# 모델 로드 및 LoRA 구성 적용\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 하이퍼파라미터 가져오기 및 데이터 준비\n",
    "num_epochs, batch_size, learning_rate = get_hyperparameters()\n",
    "train_loader, test_loader = download_and_prepare_data(data_url, tokenizer, batch_size)\n",
    "\n",
    "# 옵티마이저 초기화\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 훈련 루프\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"에포크 {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for input_ids, attention_mask, labels, _, _ in progress_bar:\n",
    "        # 배치를 장치로 이동\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 포워드 패스\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # 백워드 패스 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 메트릭 업데이트\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        progress_bar.set_postfix({\"손실\": total_loss / num_batches})\n",
    "\n",
    "    # 에포크 메트릭 계산 및 표시\n",
    "    avg_loss = total_loss / num_batches\n",
    "    test_acc = calculate_accuracy(model, tokenizer, test_loader)\n",
    "    print(f\"에포크 {epoch+1} - 평균 손실: {avg_loss:.4f}, 테스트 정확도: {test_acc:.4f}\")\n",
    "\n",
    "# 최종 모델 성능 계산\n",
    "train_acc = calculate_accuracy(model, tokenizer, train_loader)\n",
    "print(f\"훈련 정확도: {train_acc:.4f}\")\n",
    "print(f\"테스트 정확도: {test_acc:.4f}\")\n",
    "\n",
    "# LoRA 미세 튜닝된 모델과 토크나이저 저장\n",
    "model.save_pretrained(\"./finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model\")\n",
    "\n",
    "# 샘플 입력으로 미세 튜닝된 모델 테스트\n",
    "test_input = \"I'm so happy to be able to finetune an LLM!\"\n",
    "test_model(\"./finetuned_model\", test_input)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
