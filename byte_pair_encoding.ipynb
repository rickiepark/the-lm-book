{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickiepark/the-lm-book/blob/main/byte_pair_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"display: flex; justify-content: center;\">\n",
        "    <div style=\"background-color: #f4f6f7; padding: 15px; width: 80%;\">\n",
        "        <table style=\"width: 100%\">\n",
        "            <tr>\n",
        "                <td style=\"vertical-align: middle;\">\n",
        "                    <span style=\"font-size: 14px;\">\n",
        "                        A notebook for <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">The Hundred-Page Language Models Book</a> by Andriy Burkov<br><br>\n",
        "                        Code repository: <a href=\"https://github.com/rickiepark/the-lm-book\" target=\"_blank\" rel=\"noopener\">https://github.com/rickiepark/the-lm-book</a>\n",
        "                    </span>\n",
        "                </td>\n",
        "                <td style=\"vertical-align: middle;\">\n",
        "                    <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">\n",
        "                        <img src=\"https://thelmbook.com/img/book.png\" width=\"80px\" alt=\"The Hundred-Page Language Models Book\">\n",
        "                    </a>\n",
        "                </td>\n",
        "            </tr>\n",
        "        </table>\n",
        "    </div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "tRiU14bopExj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE 모델 훈련하기"
      ],
      "metadata": {
        "id": "btXb1IGF0r6S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yy0zjL_2ouOU"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 임포트\n",
        "import os  # 파일과 경로 처리\n",
        "import urllib.request  # 파일 다운로드\n",
        "import tarfile  # tar 파일 추출\n",
        "import pickle  # 토크나이저 저장 및 로딩\n",
        "import re  # 정규표현식\n",
        "import time  # 시간 계산\n",
        "from collections import defaultdict  # 토큰 및 쌍 카운트\n",
        "\n",
        "def download_file(url, filename):\n",
        "    \"\"\"\n",
        "    중복 다운로드를 막기 위해 파일 존재 여부를 확인하여 로컬에 없는 경우 URL에서 다운로드합니다.\n",
        "\n",
        "    매개변수:\n",
        "        url (str): 다운로드할 파일의 URL\n",
        "        filename (str): 다운로드된 파일을 저장할 로컬 경로\n",
        "\n",
        "    반환값:\n",
        "        None: 다운로드 과정에 대한 상태 메시지를 출력합니다.\n",
        "    \"\"\"\n",
        "    # 중복 다운로드를 막기 위해 파일이 존재하는지 확인합니다.\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"{url}에서 파일을 다운로드합니다...\")\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "        print(\"다운로드 완료.\")\n",
        "    else:\n",
        "        print(f\"{filename}이 이미 다운로드되어 있습니다.\")\n",
        "\n",
        "def is_within_directory(directory, target):\n",
        "    \"\"\"\n",
        "    경로 탐색 공격(path traversal attack)을 막기 위해\n",
        "    타깃 경로를 확인하는 보안 검사를 수행합니다.\n",
        "    추출된 파일이 의도한 디렉토리 안에 있도록 보장합니다.\n",
        "\n",
        "    매개변수:\n",
        "        directory (str): 베이스 디렉토리 경로\n",
        "        target (str): 타깃 경로\n",
        "\n",
        "    반환값:\n",
        "        bool: target이 directory 안에 있으면 True, 그렇지 않으면 False\n",
        "    \"\"\"\n",
        "    # 비교를 위해 두 경로를 절대 경로로 바꿉니다.\n",
        "    abs_directory = os.path.abspath(directory)\n",
        "    abs_target = os.path.abspath(target)\n",
        "    # 포함 관계를 확인하기 위해 공통 경로를 추출합니다.\n",
        "    prefix = os.path.commonprefix([abs_directory, abs_target])\n",
        "    return prefix == abs_directory\n",
        "\n",
        "def safe_extract_tar(tar_file, required_files):\n",
        "    \"\"\"\n",
        "    보안 검사를 통해 tar 파일을 안전하게 추출합니다.\n",
        "    경로 탐색 공격을 방지하고 필요한 파일만 추출합니다.\n",
        "\n",
        "    매개변수:\n",
        "        tar_file (str): tar 압축 파일 경로\n",
        "        required_files (list): 추출할 파일이름 리스트\n",
        "\n",
        "    반환값:\n",
        "        None: 파일을 추출하고 과정을 출력합니다.\n",
        "\n",
        "    예외:\n",
        "        Exception: 경로 탐색 공격이 감지되는 경우\n",
        "    \"\"\"\n",
        "    with tarfile.open(tar_file, \"r:gz\") as tar:\n",
        "        # 압축된 모든 파일에 대해 보안 검사를 수행합니다.\n",
        "        for member in tar.getmembers():\n",
        "            if not is_within_directory('.', member.name):\n",
        "                raise Exception(\"Tar 파일에서 경로 탐색 공격이 감지되었습니다.\")\n",
        "\n",
        "        # 지정된 파일만 추출합니다.\n",
        "        for member in tar.getmembers():\n",
        "            if any(member.name.endswith(file) for file in required_files):\n",
        "                # 안전을 위해 파일에서 경로를 삭제합니다.\n",
        "                member.name = os.path.basename(member.name)\n",
        "                tar.extract(member, '.')\n",
        "                print(f\"{member.name} 추출\")\n",
        "\n",
        "def create_word_generator(filepath):\n",
        "    \"\"\"\n",
        "    텍스트 파일에서 한 번에 하나의 단어를 반환하는 제너레이터를 만듭니다.\n",
        "    대규모 텍스트 파일을 메모리 효율적으로 처리하는 방법입니다.\n",
        "\n",
        "    매개변수:\n",
        "        filepath (str): 텍스트 파일 경로\n",
        "\n",
        "    반환값:\n",
        "        generator: 파일에 있는 개별 단어\n",
        "    \"\"\"\n",
        "    def generator():\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f:\n",
        "                for word in line.split():\n",
        "                    yield word\n",
        "    return generator()\n",
        "\n",
        "def download_and_prepare_data(url):\n",
        "    \"\"\"\n",
        "    훈련 데이터셋을 다운로드, 추출, 준비합니다.\n",
        "    다운로드와 보안 검사를 포함한 데이터 추출을 처리합니다.\n",
        "\n",
        "    매개변수:\n",
        "        url (str): 다운로드할 데이터셋 URL\n",
        "\n",
        "    반환값:\n",
        "        generator: 훈련 데이터를 위한 단어 제너레이터\n",
        "    \"\"\"\n",
        "    required_files = [\"train.txt\", \"test.txt\"]\n",
        "    filename = os.path.basename(url)\n",
        "\n",
        "    # 필요한 경우 데이터셋을 다운로드합니다.\n",
        "    download_file(url, filename)\n",
        "\n",
        "    # 기존에 없는 경우 필요한 파일을 추출합니다.\n",
        "    if not all(os.path.exists(file) for file in required_files):\n",
        "        print(\"파일 추출...\")\n",
        "        safe_extract_tar(filename, required_files)\n",
        "        print(\"추출 완료.\")\n",
        "    else:\n",
        "        print(\"'train.txt'와 'test.txt'는 이미 추출되어 있습니다.\")\n",
        "\n",
        "    # 단어 제너레이터를 만들어 반환합니다.\n",
        "    return create_word_generator(\"train.txt\")\n",
        "\n",
        "def initialize_vocabulary(corpus):\n",
        "    \"\"\"\n",
        "    단어를 문자로 분할하여 말뭉치에서 초기 어휘사전을 만듭니다.\n",
        "    단어 경계 문자 '_'를 추가하고 고유한 문자를 추적합니다.\n",
        "\n",
        "    매개변수:\n",
        "        corpus (iterable): 처리할 단어의 반복자 또는 리스트\n",
        "\n",
        "    반환값:\n",
        "        tuple: (토큰화된 단어를 카운트에 매핑하는 어휘사전 딕셔너리, 말뭉치에 있는 고유한 문자 집합)\n",
        "    \"\"\"\n",
        "    # 단어 카운트와 고유한 문자를 추적합니다.\n",
        "    vocabulary = defaultdict(int)\n",
        "    charset = set()\n",
        "\n",
        "    for word in corpus:\n",
        "        # 단어 경계 문자를 추가하고 문자로 분할합니다.\n",
        "        word_with_marker = '_' + word\n",
        "        characters = list(word_with_marker)\n",
        "        # 고유 문자 집합을 업데이트합니다.\n",
        "        charset.update(characters)\n",
        "        # 공백으로 구분된 문자의 문자열을 만듭니다.\n",
        "        tokenized_word = \" \".join(characters)\n",
        "        # 토큰화된 단어의 카운트를 증가시킵니다.\n",
        "        vocabulary[tokenized_word] += 1\n",
        "\n",
        "    return vocabulary, charset\n",
        "\n",
        "def get_pair_counts(vocabulary):\n",
        "    \"\"\"\n",
        "    어휘사전에 인접한 기호 쌍의 빈도를 카운트합니다.\n",
        "    가장 빈번한 쌍을 식별하여 합치는데 사용합니다.\n",
        "\n",
        "    매개변수:\n",
        "        vocabulary (dict): 토큰화된 단어와 카운트를 매핑하는 딕셔너리\n",
        "\n",
        "    Returns:\n",
        "        defaultdict: 토큰 쌍과 빈도 카운트의 매핑\n",
        "    \"\"\"\n",
        "    pair_counts = defaultdict(int)\n",
        "    for tokenized_word, count in vocabulary.items():\n",
        "        # 단어를 토큰으로 분할합니다.\n",
        "        tokens = tokenized_word.split()\n",
        "        # 단어 빈도로 인접 쌍의 카운트를 할당합니다.\n",
        "        for i in range(len(tokens) - 1):\n",
        "            pair = (tokens[i], tokens[i + 1])\n",
        "            pair_counts[pair] += count\n",
        "    return pair_counts\n",
        "\n",
        "def merge_pair(vocab, pair):\n",
        "    \"\"\"\n",
        "    어휘사전에 있는 특정 기호 쌍을 합칩니다.\n",
        "    토큰 경계를 정확하게 매치하기 위해 정규 표현식을 사용합니다.\n",
        "\n",
        "    매개변수:\n",
        "        vocab (dict): 현재 어휘사전 딕셔너리\n",
        "        pair (tuple): 병합된 토큰 쌍\n",
        "\n",
        "    반환값:\n",
        "        dict: 특정 쌍이 병합된 새로운 어휘사전\n",
        "    \"\"\"\n",
        "    new_vocab = {}\n",
        "    # 바이그램을 매칭하기 위해 정규 표현식을 만듭니다.\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    pattern = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
        "\n",
        "    # 어휘사전에 있는 모든 단어를 병합합니다.\n",
        "    for tokenized_word, count in vocab.items():\n",
        "        new_tokenized_word = pattern.sub(\"\".join(pair), tokenized_word)\n",
        "        new_vocab[new_tokenized_word] = count\n",
        "    return new_vocab\n",
        "\n",
        "def byte_pair_encoding(corpus, vocab_size):\n",
        "    \"\"\"\n",
        "    부분단어 어휘사전을 학습하기 위해 BPE 알고리즘을 구현합니다.\n",
        "    목표하는 어휘사전 크기에 도달할 때까지 가장 빈번하게 등장하는 문자 쌍을 반복적으로 병합합니다.\n",
        "\n",
        "    매개변수:\n",
        "        corpus (iterable): BPE가 학습할 단어의 반복자 또는 리스트\n",
        "        vocab_size (int): 병합을 중지할 목표 어휘사전 크기\n",
        "\n",
        "    반환값:\n",
        "        tuple: (최종 어휘사전 딕셔너리, 병합 연산 리스트,\n",
        "               기본 문자 집합, 모든 토큰 집합)\n",
        "    \"\"\"\n",
        "    # 문자 수준 토큰으로 어휘사전을 초기화합니다.\n",
        "    vocab, charset = initialize_vocabulary(corpus)\n",
        "    merges = []\n",
        "    tokens = set(charset)\n",
        "\n",
        "    # 목표 어휘사전 크기에 도달할 때까지 계속 쌍을 병합합니다.\n",
        "    while len(tokens) < vocab_size:\n",
        "        # 인접한 모든 토큰 쌍의 카운트를 얻습니다.\n",
        "        pair_counts = get_pair_counts(vocab)\n",
        "        if not pair_counts:\n",
        "            break\n",
        "\n",
        "        # 가장 많이 등장하는 쌍을 찾아 기록합니다.\n",
        "        most_frequent_pair = max(pair_counts, key=pair_counts.get)\n",
        "        merges.append(most_frequent_pair)\n",
        "\n",
        "        # 가장 많이 등장하는 쌍을 병합하여 어휘사전을 업데이트합니다.\n",
        "        vocab = merge_pair(vocab, most_frequent_pair)\n",
        "\n",
        "        # 새롭게 병합된 토큰을 토큰 집합에 추가합니다.\n",
        "        new_token = \"\".join(most_frequent_pair)\n",
        "        tokens.add(new_token)\n",
        "\n",
        "    return vocab, merges, charset, tokens\n",
        "\n",
        "def tokenize_word(word, merges, charset, unk_token=\"<UNK>\"):\n",
        "    \"\"\"\n",
        "    학습된 BPE 병합을 사용해 단일 단어를 토큰화합니다.\n",
        "    알지 못하는 문자는 UNK 토큰으로 처리합니다.\n",
        "\n",
        "    매개변수:\n",
        "        word (str): 토큰화할 단어\n",
        "        merges (list): 학습된 병합 연산 리스트\n",
        "        charset (set): 알려진 문자 집합\n",
        "        unk_token (str): 알지 못하는 문자를 대체할 토큰\n",
        "\n",
        "    반환값:\n",
        "        list: 해당 단어에 대한 토큰 리스트\n",
        "    \"\"\"\n",
        "    # 단어 경계 문자를 추가하고 문자로 분할합니다.\n",
        "    word = '_' + word\n",
        "    tokens = [char if char in charset else unk_token for char in word]\n",
        "\n",
        "    # 순서대로 병합 연산을 적용합니다.\n",
        "    for left, right in merges:\n",
        "        i = 0\n",
        "        while i < len(tokens) - 1:\n",
        "            if tokens[i:i+2] == [left, right]:\n",
        "                tokens[i:i+2] = [left + right]\n",
        "            else:\n",
        "                i += 1\n",
        "    return tokens\n",
        "\n",
        "def build_merge_map(merges):\n",
        "    \"\"\"\n",
        "    토큰 쌍을 병합 결과에 매핑하는 딕셔너리를 만듭니다.\n",
        "    일관된 토큰화를 위해 병합 순서를 유지합니다.\n",
        "\n",
        "    매개변수:\n",
        "        merges (list): 병합 연산의 리스트\n",
        "\n",
        "    반환값:\n",
        "        dict: 병합 쌍과 (병합된 토큰, 병합 우선순위) 튜플의 매핑\n",
        "    \"\"\"\n",
        "    merge_map = {}\n",
        "    # 병합 우선순위로 매핑을 만듭니다.\n",
        "    for i, (left, right) in enumerate(merges):\n",
        "        merged_token = left + right\n",
        "        merge_map[(left, right)] = (merged_token, i)\n",
        "    return merge_map\n",
        "\n",
        "def tokenize_word_fast(word, merge_map, vocabulary, charset, unk_token=\"<UNK>\"):\n",
        "    \"\"\"\n",
        "    미리 계산된 병합 매핑을 사용해 토큰화 함수를 최적화합니다.\n",
        "    원본 알고리즘과 동일하지만 더 빠르게 결과를 생성합니다.\n",
        "\n",
        "    매개변수:\n",
        "        word (str): 토큰화할 단어\n",
        "        merge_map (dict): 토큰 쌍과 병합 결과의 매핑\n",
        "        vocabulary (dict): 현재 어휘사전 딕셔너리\n",
        "        charset (set): 알려진 문자 집합\n",
        "        unk_token (str): 알지 못하는 문자를 대체할 토큰\n",
        "\n",
        "    반환값:\n",
        "        list: 해당 단어의 토큰 리스트\n",
        "    \"\"\"\n",
        "    # 단어가 어휘사전에 있는지 확인합니다.\n",
        "    word_with_prefix = '_' + word\n",
        "    if word_with_prefix in vocabulary:\n",
        "        return [word_with_prefix]\n",
        "\n",
        "    # 단어를 문자로 분할하고 알지 못하는 문자를 대체합니다.\n",
        "    tokens = [char if char in charset else unk_token for char in word_with_prefix]\n",
        "\n",
        "    # 더 이상 가능한 병합이 없을 때까지 병합을 계속합니다.\n",
        "    while True:\n",
        "        # 가능한 모든 병합 연산을 찾습니다.\n",
        "        pairs_with_positions = []\n",
        "        for i in range(len(tokens) - 1):\n",
        "            pair = (tokens[i], tokens[i + 1])\n",
        "            if pair in merge_map:\n",
        "                merged_token, merge_priority = merge_map[pair]\n",
        "                pairs_with_positions.append((i, pair, merged_token, merge_priority))\n",
        "\n",
        "        # 더 이상 가능한 병합이 없으면 종룟합니다.\n",
        "        if not pairs_with_positions:\n",
        "            break\n",
        "\n",
        "        # 일관성을 위해 병합 우선순위와 위치로 정렬합니다.\n",
        "        pairs_with_positions.sort(key=lambda x: (x[3], x[0]))\n",
        "\n",
        "        # 유효한 첫 번째 병합을 적용합니다.\n",
        "        pos, pair, merged_token, _ = pairs_with_positions[0]\n",
        "        tokens[pos:pos+2] = [merged_token]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def save_tokenizer(merges, charset, tokens, filename=\"tokenizer.pkl\"):\n",
        "    \"\"\"\n",
        "    나중을 위해 토크나이저 상태를 피클 파일에 저장합니다.\n",
        "\n",
        "    매개변수:\n",
        "        merges (list): 병합 연산 리스트\n",
        "        charset (set): 알려진 문자 집합\n",
        "        tokens (set): 모든 토큰 집합\n",
        "        filename (str): 토크나이저 상태를 저장할 경로\n",
        "\n",
        "    반환값:\n",
        "        None: 토크나이저를 디스크에 저장합니다.\n",
        "    \"\"\"\n",
        "    with open(filename, \"wb\") as f:\n",
        "        pickle.dump({\n",
        "            \"merges\": merges,\n",
        "            \"charset\": charset,\n",
        "            \"tokens\": tokens\n",
        "        }, f)\n",
        "\n",
        "def load_tokenizer(filename=\"tokenizer.pkl\"):\n",
        "    \"\"\"\n",
        "    피클 파일에서 토크나이저 상태를 로드합니다.\n",
        "\n",
        "    매개변수:\n",
        "        filename (str): 토크나이저 상태가 저장된 경로\n",
        "\n",
        "    반환값:\n",
        "        dict: 토크나이저 구성요소가 담긴 딕셔너리\n",
        "    \"\"\"\n",
        "    with open(filename, \"rb\") as f:\n",
        "        return pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 설정 파라미터\n",
        "vocab_size = 5_000  # 어휘사전 크기\n",
        "max_corpus_size = 500_000  # 처리할 최대 단어 개수\n",
        "data_url = \"https://www.thelmbook.com/data/news\"  # 데이터셋\n",
        "\n",
        "# 훈련 데이터를 다운로드하고 전처리합니다\n",
        "word_gen = download_and_prepare_data(data_url)\n",
        "\n",
        "# 말뭉치가 최대 크기에 도달할 때까지 단어를 모읍니다.\n",
        "word_list = []\n",
        "for word in word_gen:\n",
        "    word_list.append(word)\n",
        "    if len(word_list) >= max_corpus_size:\n",
        "        break\n",
        "\n",
        "# BPE 토크나이저를 훈련합니다.\n",
        "print(\"BPE 토크나이저 훈련...\")\n",
        "vocab, merges, charset, tokens = byte_pair_encoding(word_list, vocab_size)\n",
        "\n",
        "# 훈련된 토크나이저를 저장합니다.\n",
        "print(\"토크나이저 저장...\")\n",
        "save_tokenizer(merges, charset, tokens)"
      ],
      "metadata": {
        "id": "9T9BnhfpQRJG",
        "outputId": "7e173925-6960-4599-eee5-ff07dd9cd7e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.thelmbook.com/data/news에서 파일을 다운로드합니다...\n",
            "다운로드 완료.\n",
            "파일 추출...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4014581585.py:75: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extract(member, '.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.txt 추출\n",
            "test.txt 추출\n",
            "추출 완료.\n",
            "BPE 토크나이저 훈련...\n",
            "토크나이저 저장...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 훈련된 BPE 토크나이저 테스트\n",
        "\n",
        "토크나이저가 훈련되고 나면 이를 로드하여 새로운 텍스트에 적용할 수 있습니다:"
      ],
      "metadata": {
        "id": "NXvPpOgo0Wgk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0_RrAhuF7_B",
        "outputId": "6cdc3a21-da71-417a-9896-e5d1b8438c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "토크나이저 로딩...\n",
            "\n",
            "일반적인 구현으로 토큰화한 문장:\n",
            "Let's -> ['_Let', \"'\", 's']\n",
            "proceed -> ['_proceed']\n",
            "to -> ['_to']\n",
            "the -> ['_the']\n",
            "language -> ['_language']\n",
            "modeling -> ['_model', 'ing']\n",
            "part. -> ['_part', '.']\n",
            "--- 소요시간: 0.016661643981933594 초 ---\n",
            "\n",
            "빠른 구현으로 토큰화한 문장:\n",
            "Let's -> ['_Let', \"'\", 's']\n",
            "proceed -> ['_proceed']\n",
            "to -> ['_to']\n",
            "the -> ['_the']\n",
            "language -> ['_language']\n",
            "modeling -> ['_model', 'ing']\n",
            "part. -> ['_part', '.']\n",
            "--- 소요시간: 0.0004608631134033203 초 ---\n",
            "\n",
            "어휘사전 크기: 5000\n"
          ]
        }
      ],
      "source": [
        "print(\"토크나이저 로딩...\")\n",
        "tokenizer = load_tokenizer()\n",
        "\n",
        "# 로딩된 토크나이저로 샘플 문장을 토큰화합니다.\n",
        "sentence = \"Let's proceed to the language modeling part.\"\n",
        "\n",
        "start_time = time.time()\n",
        "tokenized_sentence = [tokenize_word(word, tokenizer[\"merges\"], tokenizer[\"charset\"]) for word in sentence.split()]\n",
        "elapsed = time.time() - start_time\n",
        "print(\"\\n일반적인 구현으로 토큰화한 문장:\")\n",
        "for word, tokens in zip(sentence.split(), tokenized_sentence):\n",
        "    print(f\"{word} -> {tokens}\")\n",
        "print(\"--- 소요시간: %s 초 ---\" % (elapsed))\n",
        "\n",
        "merge_map = build_merge_map(tokenizer[\"merges\"])\n",
        "start_time = time.time()\n",
        "fast_tokenized_sentence = [tokenize_word_fast(word, merge_map, vocab, tokenizer[\"charset\"]) for word in sentence.split()]\n",
        "elapsed = time.time() - start_time\n",
        "print(\"\\n빠른 구현으로 토큰화한 문장:\")\n",
        "for word, tokens in zip(sentence.split(), fast_tokenized_sentence):\n",
        "    print(f\"{word} -> {tokens}\")\n",
        "print(\"--- 소요시간: %s 초 ---\" % (time.time() - start_time))\n",
        "\n",
        "print(\"\\n어휘사전 크기:\", len(tokenizer[\"tokens\"]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}