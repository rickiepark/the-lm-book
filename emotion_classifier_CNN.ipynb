{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickiepark/the-lm-book/blob/main/emotion_classifier_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yy0zjL_2ouOU"
      },
      "outputs": [],
      "source": [
        "# 필수 라이브러리 임포트\n",
        "import torch                             # 텐서 연산과 딥러닝을 위한 파이토치\n",
        "import torch.nn as nn                    # 파이토치의 신경망 모듈\n",
        "import numpy as np                       # 수치 연산을 위한 NumPy\n",
        "import re                                # 텍스트 처리를 위한 정규 표현식 (필요한 경우)\n",
        "import gzip                              # 압축 파일 처리를 위한 라이브러리\n",
        "import json                              # JSON 데이터 구문 분석을 위한 라이브러리\n",
        "import requests                          # 데이터 다운로드를 위한 HTTP 요청 라이브러리\n",
        "import random                            # 데이터 셔플링 및 난수 시드 설정을 위한 라이브러리\n",
        "import pickle                            # 직렬화된 객체 저장 및 로딩을 위한 라이브러리\n",
        "import os                                # 파일 시스템 작업을 위한 라이브러리\n",
        "from tqdm import tqdm                    # 루프 중 진행률 표시를 위한 라이브러리\n",
        "from torch.utils.data import Dataset, DataLoader  # 파이토치에서 사용자 정의 데이터셋 및 데이터 로더 생성을 위한 라이브러리\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    다양한 라이브러리에서 재현성을 위해 난수 시드를 설정합니다.\n",
        "    Args:\n",
        "        seed (int): 난수 생성을 위한 시드 값.\n",
        "    \"\"\"\n",
        "    random.seed(seed)                      # 파이썬 내장 random 모듈의 시드 설정\n",
        "    torch.manual_seed(seed)                # 파이토치 CPU 연산을 위한 시드 설정\n",
        "    torch.cuda.manual_seed_all(seed)       # 모든 GPU에 대한 시드 설정\n",
        "    torch.backends.cudnn.deterministic = True  # cuDNN에 결정론적 알고리즘 사용\n",
        "    torch.backends.cudnn.benchmark = False     # 일관된 동작을 위해 cuDNN 자동 튜너 비활성화\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\"\n",
        "    공백을 기준으로 텍스트를 분할하는 기본 토크나이저.\n",
        "    \"\"\"\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"\n",
        "        입력 텍스트를 공백을 기준으로 토큰으로 분할합니다.\n",
        "        Args:\n",
        "            text (str): 토큰화할 입력 텍스트 문자열.\n",
        "        Returns:\n",
        "            list: 단어 토큰의 목록.\n",
        "        \"\"\"\n",
        "        words = text.split()             # 공백을 기준으로 텍스트 분할\n",
        "        return words\n",
        "\n",
        "class Embedder:\n",
        "    \"\"\"\n",
        "    토큰을 해당 임베딩으로 변환하는 클래스.\n",
        "    Attributes:\n",
        "        embeddings (dict): 단어에서 벡터 표현으로의 매핑 딕셔너리.\n",
        "        emb_dim (int): 임베딩의 차원.\n",
        "        seq_len (int): 입력 텍스트의 고정 시퀀스 길이.\n",
        "    \"\"\"\n",
        "    def __init__(self, embeddings, emb_dim, seq_len):\n",
        "        \"\"\"\n",
        "        임베딩, 임베딩 차원 및 시퀀스 길이로 Embedder를 초기화합니다.\n",
        "        Args:\n",
        "            embeddings (dict): 미리 로드된 단어 임베딩.\n",
        "            emb_dim (int): 임베딩의 차원.\n",
        "            seq_len (int): 고려할 최대 토큰 수.\n",
        "        \"\"\"\n",
        "        self.embeddings = embeddings\n",
        "        self.emb_dim = emb_dim\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def embed(self, tokens):\n",
        "        \"\"\"\n",
        "        토큰 목록을 임베딩 텐서로 변환합니다.\n",
        "        토큰은 임베딩 딕셔너리에서 조회됩니다. 토큰을 찾을 수 없는 경우,\n",
        "        영 벡터가 사용됩니다. 시퀀스는 고정 시퀀스 길이에 맞춰 잘리거나 패딩됩니다.\n",
        "        Args:\n",
        "            tokens (list): 토큰 문자열 목록.\n",
        "        Returns:\n",
        "            torch.Tensor: 임베딩된 토큰을 나타내는 (seq_len, emb_dim) 크기의 텐서.\n",
        "        \"\"\"\n",
        "        embeddings = []\n",
        "        # 최대 시퀀스 길이까지 각 토큰 처리\n",
        "        for word in tokens[:self.seq_len]:\n",
        "            if word in self.embeddings:\n",
        "                embeddings.append(torch.tensor(self.embeddings[word]))\n",
        "            elif word.lower() in self.embeddings:\n",
        "                embeddings.append(torch.tensor(self.embeddings[word.lower()]))\n",
        "            else:\n",
        "                # 임베딩에서 찾을 수 없는 단어에는 영 벡터 사용\n",
        "                embeddings.append(torch.zeros(self.emb_dim))\n",
        "        # 토큰 수가 seq_len보다 작은 경우 영 벡터로 시퀀스 패딩\n",
        "        if len(embeddings) < self.seq_len:\n",
        "            padding_size = self.seq_len - len(embeddings)\n",
        "            embeddings.extend([torch.zeros(self.emb_dim)] * padding_size)\n",
        "        # 텐서 목록을 (seq_len, emb_dim) 크기의 단일 텐서로 쌓기\n",
        "        return torch.stack(embeddings)\n",
        "\n",
        "def download_file(url, filename, chunk_size=8192):\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"파일 {filename}이 이미 존재합니다. 다운로드를 건너뜁니다.\")\n",
        "        return\n",
        "\n",
        "    # 세션 생성 및 재시도 설정\n",
        "    session = requests.Session()\n",
        "    retries = Retry(\n",
        "        total=5,              # 총 5번 재시도\n",
        "        backoff_factor=1,     # 점점 늘어나는 대기시간 (1s, 2s, 4s…)\n",
        "        status_forcelist=[500, 502, 503, 504]\n",
        "    )\n",
        "    session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "    session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "\n",
        "    with session.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        total_size = int(r.headers.get(\"content-length\", 0))\n",
        "\n",
        "        with tqdm(total=total_size, unit=\"B\", unit_scale=True, unit_divisor=1024, desc=\"다운로드 중\") as progress_bar:\n",
        "            with open(filename, \"wb\") as f:\n",
        "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "                        progress_bar.update(len(chunk))\n",
        "\n",
        "    print(f\"다운로드 완료: {filename}\")\n",
        "\n",
        "def load_embeddings(url, filename=\"vectors.dat\"):\n",
        "    \"\"\"\n",
        "    gzip 압축 파일에서 단어 임베딩을 다운로드하고 로드합니다.\n",
        "    파일이 로컬에 존재하지 않는 경우, 제공된 URL에서 다운로드됩니다.\n",
        "    파일은 어휘 크기와 임베딩 차원을 나타내는 헤더를 가질 것으로 예상되며,\n",
        "    그 뒤에 각 단어와 해당 바이너리 벡터가 이어집니다.\n",
        "    Args:\n",
        "        url (str): 임베딩을 다운로드할 URL.\n",
        "        filename (str): 임베딩을 저장/로드할 로컬 파일명.\n",
        "    Returns:\n",
        "        tuple: 다음을 포함하는 튜플:\n",
        "            - vectors (dict): 단어에서 임베딩 벡터(NumPy 배열로)로의 매핑.\n",
        "            - emb_dim (int): 임베딩 벡터의 차원.\n",
        "    \"\"\"\n",
        "    # 임베딩 파일이 로컬에 존재하는지 확인한 후 다운로드\n",
        "    download_file(url, filename, chunk_size=1024)\n",
        "\n",
        "    # 바이너리 읽기 모드로 gzip 압축 임베딩 파일 열기\n",
        "    with gzip.open(filename, \"rb\") as f:\n",
        "        # 헤더 라인을 읽어 어휘 크기와 임베딩 차원 가져오기\n",
        "        header = f.readline()\n",
        "        vocab_size, emb_dim = map(int, header.split())\n",
        "        vectors = {}\n",
        "        # 각 임베딩 벡터의 바이트 수 계산\n",
        "        binary_len = np.dtype(\"float32\").itemsize * emb_dim\n",
        "        # 진행률 표시줄과 함께 각 단어와 해당 임베딩 벡터 읽기\n",
        "        with tqdm(total=vocab_size, desc=\"단어 벡터 로딩 중\") as pbar:\n",
        "            for _ in range(vocab_size):\n",
        "                word = []\n",
        "                # 공백이 나타날 때까지(단어 끝을 나타냄) 문자를 하나씩 읽기\n",
        "                while True:\n",
        "                    ch = f.read(1)\n",
        "                    if ch == b\" \":\n",
        "                        word = b\"\".join(word).decode(\"utf-8\")\n",
        "                        break\n",
        "                    if ch != b\"\\n\":\n",
        "                        word.append(ch)\n",
        "                # 바이너리 벡터 데이터를 읽고 float32 유형의 NumPy 배열로 변환\n",
        "                vector = np.frombuffer(f.read(binary_len), dtype=\"float32\")\n",
        "                vectors[word] = vector\n",
        "                pbar.update(1)\n",
        "    return vectors, emb_dim\n",
        "\n",
        "def load_and_split_data(url, test_ratio=0.1):\n",
        "    \"\"\"\n",
        "    데이터셋을 다운로드, 압축 해제 및 훈련 및 테스트 세트로 분할합니다.\n",
        "    데이터셋은 각 라인이 JSON 객체인 gzip 파일로 예상됩니다.\n",
        "    Args:\n",
        "        url (str): 데이터셋을 다운로드할 URL.\n",
        "        test_ratio (float): 테스트 세트로 사용할 데이터 비율.\n",
        "    Returns:\n",
        "        tuple: 다음을 포함하는 튜플:\n",
        "            - train_data (list): 훈련 예시 목록.\n",
        "            - test_data (list): 테스트 예시 목록.\n",
        "    \"\"\"\n",
        "    # 제공된 URL에서 데이터셋 다운로드\n",
        "    response = requests.get(url)\n",
        "    # gzip 압축 콘텐츠 압축 해제 및 문자열로 디코딩\n",
        "    content = gzip.decompress(response.content).decode()\n",
        "    # 각 라인을 JSON 객체로 구문 분석\n",
        "    data = [json.loads(line) for line in content.splitlines()]\n",
        "    # 무작위 분포를 보장하기 위해 데이터 셔플\n",
        "    random.shuffle(data)\n",
        "    # test_ratio를 기반으로 분할 인덱스 결정\n",
        "    split_index = int(len(data) * (1 - test_ratio))\n",
        "    return data[:split_index], data[split_index:]\n",
        "\n",
        "def download_and_prepare_data(data_url, vectors_url, seq_len, batch_size):\n",
        "    \"\"\"\n",
        "    훈련 및 평가를 위해 데이터셋과 단어 임베딩을 다운로드하고 준비합니다.\n",
        "    이 함수는 텍스트 데이터셋과 단어 임베딩을 다운로드하고, 레이블 매핑을 생성하며,\n",
        "    토크나이저와 Embedder를 초기화하고, 훈련 및 테스트를 위한 데이터 로더를 반환합니다.\n",
        "    Args:\n",
        "        data_url (str): 텍스트 데이터셋을 다운로드할 URL.\n",
        "        vectors_url (str): 단어 임베딩을 다운로드할 URL.\n",
        "        seq_len (int): 토큰 임베딩의 고정 시퀀스 길이.\n",
        "        batch_size (int): 데이터 로더의 배치 크기.\n",
        "    Returns:\n",
        "        tuple: 다음을 포함하는 튜플:\n",
        "            - train_loader (DataLoader): 훈련 데이터용 DataLoader.\n",
        "            - test_loader (DataLoader): 테스트 데이터용 DataLoader.\n",
        "            - id_to_label (dict): 레이블 ID에서 레이블 이름으로의 매핑.\n",
        "            - num_classes (int): 고유 클래스 수.\n",
        "            - emb_dim (int): 단어 임베딩의 차원.\n",
        "    \"\"\"\n",
        "    # 데이터셋을 훈련 및 테스트 분할로 로드 및 분할\n",
        "    train_split, test_split = load_and_split_data(data_url, test_ratio=0.1)\n",
        "    # 사전 훈련된 단어 임베딩 로드 및 임베딩 차원 가져오기\n",
        "    embeddings, emb_dim = load_embeddings(vectors_url)\n",
        "    # 훈련 데이터를 사용하여 레이블과 숫자 ID 사이의 매핑 생성\n",
        "    label_to_id, id_to_label, num_classes = create_label_mappings(train_split)\n",
        "    # 로드된 임베딩과 시퀀스 길이로 토크나이저와 Embedder 초기화\n",
        "    tokenizer = Tokenizer()\n",
        "    embedder = Embedder(embeddings, emb_dim, seq_len)\n",
        "    # 훈련 및 테스트 데이터셋 모두에 대한 DataLoader 생성\n",
        "    train_loader, test_loader = create_data_loaders(\n",
        "        train_split, test_split,\n",
        "        tokenizer, embedder,\n",
        "        label_to_id, batch_size\n",
        "    )\n",
        "    return (train_loader, test_loader, id_to_label, num_classes, emb_dim)\n",
        "\n",
        "class TextClassificationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    텍스트 분류를 위한 파이토치 데이터셋.\n",
        "    이 데이터셋은 원시 텍스트와 레이블 데이터를 모델에 공급할 수 있는 형식으로 변환합니다.\n",
        "    제공된 토크나이저와 Embedder를 사용하여 텍스트를 토큰화한 다음 임베딩합니다.\n",
        "    Args:\n",
        "        data (list): \"text\" 및 \"label\" 키를 포함하는 딕셔너리 리스트.\n",
        "        tokenizer (Tokenizer): 텍스트를 토큰으로 분할하는 토크나이저 인스턴스.\n",
        "        embedder (Embedder): 토큰을 임베딩으로 변환하는 Embedder 인스턴스.\n",
        "        label_to_id (dict): 레이블 문자열에서 숫자 ID로의 매핑.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, tokenizer, embedder, label_to_id):\n",
        "        # 텍스트 추출 및 레이블을 해당 ID로 변환\n",
        "        self.texts = [item[\"text\"] for item in data]\n",
        "        self.label_ids = [label_to_id[item[\"label\"]] for item in data]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embedder = embedder\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        데이터셋의 총 예시 수를 반환합니다.\n",
        "        \"\"\"\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        지정된 인덱스의 예시에 대한 임베딩된 텍스트와 레이블을 검색합니다.\n",
        "        Args:\n",
        "            idx (int): 검색할 예시의 인덱스.\n",
        "        Returns:\n",
        "            tuple: 다음을 포함하는 튜플:\n",
        "                - embeddings (torch.Tensor): (seq_len, emb_dim) 크기의 텐서.\n",
        "                - label (torch.Tensor): 레이블 ID를 포함하는 텐서.\n",
        "        \"\"\"\n",
        "        # 주어진 인덱스의 텍스트 토큰화\n",
        "        tokens = self.tokenizer.tokenize(self.texts[idx])\n",
        "        # Embedder를 사용하여 토큰을 임베딩으로 변환\n",
        "        embeddings = self.embedder.embed(tokens)\n",
        "        # 임베딩과 해당 레이블을 텐서로 반환\n",
        "        return embeddings, torch.tensor(self.label_ids[idx], dtype=torch.long)\n",
        "\n",
        "class CNNTextClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    텍스트 분류를 위한 합성곱 신경망.\n",
        "    이 모델은 입력 텍스트의 임베딩된 표현을 기반으로 분류하기 위해\n",
        "    두 개의 1D 합성곱 층과 그 뒤에 완전 연결 층을 적용합니다.\n",
        "    Args:\n",
        "        emb_dim (int): 입력 임베딩의 차원.\n",
        "        num_classes (int): 분류를 위한 타깃 클래스 수.\n",
        "        seq_len (int): 입력 텍스트의 고정 시퀀스 길이.\n",
        "        id_to_label (dict): 레이블 ID에서 레이블 이름으로의 매핑.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim, num_classes, seq_len, id_to_label):\n",
        "        super().__init__()\n",
        "        # 나중에 사용하기 위해 모델 구성 저장 (예: 추론 중)\n",
        "        self.config = {\n",
        "            \"emb_dim\": emb_dim,\n",
        "            \"num_classes\": num_classes,\n",
        "            \"seq_len\": seq_len,\n",
        "            \"id_to_label\": id_to_label\n",
        "        }\n",
        "        # 첫 번째 합성곱 층: 입력 채널 = emb_dim, 출력 채널 = 512\n",
        "        self.conv1 = nn.Conv1d(emb_dim, 512, kernel_size=3, padding=1)\n",
        "        # 두 번째 합성곱 층: 입력 채널 = 512, 출력 채널 = 256\n",
        "        self.conv2 = nn.Conv1d(512, 256, kernel_size=3, padding=1)\n",
        "        # 펼쳐진 특성을 128 단위로 줄이는 완전 연결 층\n",
        "        self.fc1 = nn.Linear(256 * seq_len, 128)\n",
        "        # 클래스 수에 매핑하는 출력 층\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        # ReLU 활성화 함수\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        CNN 모델의 포워드 패스를 정의합니다.\n",
        "        Args:\n",
        "            x (torch.Tensor): (batch_size, seq_len, emb_dim) 크기의 입력 텐서.\n",
        "        Returns:\n",
        "            torch.Tensor: (batch_size, num_classes) 크기의 출력 로짓 텐서.\n",
        "        \"\"\"\n",
        "        x = x.permute(0, 2, 1)           # (batch_size, emb_dim, seq_len)로 차원 재배열\n",
        "        x = self.relu(self.conv1(x))     # 첫 번째 합성곱 및 ReLU 활성화 적용\n",
        "        x = self.relu(self.conv2(x))     # 두 번째 합성곱 및 ReLU 활성화 적용\n",
        "        x = x.flatten(start_dim=1)       # 완전 연결 층을 위해 특성 펼치기\n",
        "        x = self.fc1(x)                  # 첫 번째 완전 연결 층 적용\n",
        "        return self.fc2(x)               # 최종 층에서 출력 로짓 반환\n",
        "\n",
        "def calculate_accuracy(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    제공된 데이터셋에서 모델의 정확도를 평가합니다.\n",
        "    Args:\n",
        "        model (nn.Module): 훈련된 모델.\n",
        "        dataloader (DataLoader): 평가할 데이터셋의 DataLoader.\n",
        "        device: 연산이 수행되는 장치.\n",
        "    Returns:\n",
        "        float: 정확한 예측의 비율로서의 정확도.\n",
        "    \"\"\"\n",
        "    model.eval()                       # 모델을 평가 모드로 설정\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():              # 효율성을 위해 그레이디언트 계산 비활성화\n",
        "        for batch in dataloader:\n",
        "            embeddings, labels = batch\n",
        "            embeddings = embeddings.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(embeddings)           # 포워드 패스\n",
        "            _, predicted = torch.max(outputs, 1)   # 예측된 클래스 인덱스 가져오기\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    model.train()                      # 모델을 훈련 모드로 다시 설정\n",
        "    return accuracy\n",
        "\n",
        "def create_label_mappings(train_dataset):\n",
        "    \"\"\"\n",
        "    훈련 데이터셋을 기반으로 레이블 문자열과 숫자 ID 사이의 매핑을 생성합니다.\n",
        "    Args:\n",
        "        train_dataset (list): \"label\" 필드가 있는 훈련 예시 목록.\n",
        "    Returns:\n",
        "        tuple: 다음을 포함하는 튜플:\n",
        "            - label_to_id (dict): 레이블 문자열에서 숫자 ID로의 매핑.\n",
        "            - id_to_label (dict): 숫자 ID에서 레이블 문자열로의 매핑.\n",
        "            - num_classes (int): 고유 클래스의 총 수.\n",
        "    \"\"\"\n",
        "    # 훈련 데이터에서 고유 레이블 추출 및 정렬\n",
        "    unique_labels = sorted(set(item[\"label\"] for item in train_dataset))\n",
        "    # 각 레이블을 고유 정수 ID에 매핑\n",
        "    label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
        "    # ID에서 레이블로의 역매핑 생성\n",
        "    id_to_label = {i: label for label, i in label_to_id.items()}\n",
        "    return label_to_id, id_to_label, len(unique_labels)\n",
        "\n",
        "def create_data_loaders(train_split, test_split, tokenizer, embedder, label_to_id, batch_size):\n",
        "    \"\"\"\n",
        "    훈련 및 테스트 데이터셋을 위한 파이토치 DataLoader를 생성합니다.\n",
        "    Args:\n",
        "        train_split (list): 훈련 예시 목록.\n",
        "        test_split (list): 테스트 예시 목록.\n",
        "        tokenizer (Tokenizer): 텍스트 처리를 위한 토크나이저 인스턴스.\n",
        "        embedder (Embedder): 토큰을 임베딩으로 변환하기 위한 Embedder 인스턴스.\n",
        "        label_to_id (dict): 레이블 문자열에서 숫자 ID로의 매핑.\n",
        "        batch_size (int): DataLoader의 배치 크기.\n",
        "    Returns:\n",
        "        tuple: 다음을 포함하는 튜플:\n",
        "            - train_loader (DataLoader): 훈련 데이터셋의 DataLoader.\n",
        "            - test_loader (DataLoader): 테스트 데이터셋의 DataLoader.\n",
        "    \"\"\"\n",
        "    # 훈련 및 테스트 데이터에 대한 사용자 정의 데이터셋 초기화\n",
        "    train_dataset = TextClassificationDataset(train_split, tokenizer, embedder, label_to_id)\n",
        "    test_dataset = TextClassificationDataset(test_split, tokenizer, embedder, label_to_id)\n",
        "    # 훈련 데이터에 대한 셔플링을 활성화하여 DataLoader 생성\n",
        "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def save_model(model, prefix):\n",
        "    \"\"\"\n",
        "    훈련된 모델의 상태 딕셔너리와 구성을 디스크에 저장합니다.\n",
        "    Args:\n",
        "        model (nn.Module): 저장할 훈련된 모델.\n",
        "        prefix (str): 저장된 파일 이름의 접두사.\n",
        "    \"\"\"\n",
        "    # 모델의 상태와 구성을 체크포인트 파일로 저장\n",
        "    torch.save({\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"config\": model.config\n",
        "    }, f\"{prefix}_model.pth\")\n",
        "\n",
        "def load_model(prefix):\n",
        "    \"\"\"\n",
        "    디스크에서 저장된 모델을 로드하고 평가를 위해 준비합니다.\n",
        "    Args:\n",
        "        prefix (str): 모델 저장 중 사용된 접두사.\n",
        "    Returns:\n",
        "        nn.Module: 평가 모드의 로드된 CNNTextClassifier 모델.\n",
        "    \"\"\"\n",
        "    # 모델 상태와 구성을 포함하는 체크포인트 로드\n",
        "    checkpoint = torch.load(f\"{prefix}_model.pth\", map_location=torch.device(\"cpu\"))\n",
        "    config = checkpoint[\"config\"]\n",
        "    # 저장된 구성을 사용하여 모델 재초기화\n",
        "    model = CNNTextClassifier(\n",
        "        emb_dim=config[\"emb_dim\"],\n",
        "        num_classes=config[\"num_classes\"],\n",
        "        seq_len=config[\"seq_len\"],\n",
        "        id_to_label=config[\"id_to_label\"]\n",
        "    )\n",
        "    # 저장된 가중치를 모델에 로드\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    return model\n",
        "\n",
        "def test_model(model, test_input, tokenizer=None, embedder=None):\n",
        "    \"\"\"\n",
        "    단일 입력 텍스트에서 모델을 테스트하고 예측된 레이블을 출력합니다.\n",
        "    Args:\n",
        "        model (nn.Module): 훈련된 텍스트 분류 모델.\n",
        "        test_input (str): 분류할 입력 텍스트.\n",
        "        tokenizer (Tokenizer, optional): 토크나이저 인스턴스. None인 경우 새로 생성됩니다.\n",
        "        embedder (Embedder, optional): Embedder 인스턴스. None인 경우 임베딩이 로드되고 새 Embedder 객체가 생성됩니다.\n",
        "    Notes:\n",
        "        이 함수는 입력 텍스트와 함께 예측된 감정을 출력합니다.\n",
        "    \"\"\"\n",
        "    # 제공되지 않은 경우 토크나이저 초기화\n",
        "    if not tokenizer:\n",
        "        tokenizer = Tokenizer()\n",
        "    # 제공되지 않은 경우 전역 vectors_url과 seq_len을 사용하여 임베딩 로드 및 Embedder 초기화\n",
        "    if not embedder:\n",
        "        embeddings, emb_dim = load_embeddings(vectors_url)\n",
        "        embedder = Embedder(embeddings, emb_dim, seq_len)\n",
        "    # 모델의 매개변수에서 장치 결정\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    with torch.no_grad():\n",
        "        # 입력 텍스트 토큰화\n",
        "        tokens = tokenizer.tokenize(test_input)\n",
        "        # 토큰을 임베딩으로 변환\n",
        "        embeddings = embedder.embed(tokens)\n",
        "        # 임베딩을 float 텐서로 변환, 배치 차원 추가, 올바른 장치로 이동\n",
        "        embeddings = torch.tensor(embeddings, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "        # 모델을 통한 포워드 패스로 예측 가져오기\n",
        "        outputs = model(embeddings)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 예측된 숫자 레이블을 실제 레이블 문자열로 매핑\n",
        "        predicted_label = model.config[\"id_to_label\"][predicted.item()]\n",
        "    print(f\"입력: {test_input}\")\n",
        "    print(f\"예측된 감정: {predicted_label}\")\n",
        "\n",
        "def set_hyperparameters():\n",
        "    \"\"\"\n",
        "    훈련을 위한 하이퍼파라미터를 정의하고 반환합니다.\n",
        "    Returns:\n",
        "        tuple: 다음을 포함하는 튜플:\n",
        "            - num_epochs (int): 훈련 에포크 수.\n",
        "            - seq_len (int): 입력 텍스트의 고정 시퀀스 길이.\n",
        "            - batch_size (int): 훈련 배치 크기.\n",
        "            - learning_rate (float): 옵티마이저의 학습률.\n",
        "    \"\"\"\n",
        "    num_epochs = 2\n",
        "    seq_len = 100\n",
        "    batch_size = 32\n",
        "    learning_rate = 0.001\n",
        "    return num_epochs, seq_len, batch_size, learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 재현성을 위해 난수 시드 설정\n",
        "set_seed(42)\n",
        "# 계산 장치 결정: 사용 가능한 경우 GPU 사용, 그렇지 않으면 CPU 사용\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"사용 중인 장치: {device}\")\n",
        "\n",
        "# 데이터셋과 단어 임베딩 다운로드를 위한 URL\n",
        "data_url = \"https://www.thelmbook.com/data/emotions\"\n",
        "vectors_url = \"https://www.thelmbook.com/data/word-vectors\"\n",
        "\n",
        "# 훈련 하이퍼파라미터 설정\n",
        "num_epochs, seq_len, batch_size, learning_rate = set_hyperparameters()\n",
        "\n",
        "# 데이터 로더, 레이블 매핑, 임베딩 차원 다운로드 및 준비\n",
        "train_loader, test_loader, id_to_label, num_classes, emb_dim = \\\n",
        "    download_and_prepare_data(data_url, vectors_url, seq_len, batch_size)\n",
        "\n",
        "# 임베딩 차원과 레이블 매핑으로 CNN 텍스트 분류기 모델 초기화\n",
        "model = CNNTextClassifier(emb_dim, num_classes, seq_len, id_to_label)\n",
        "model = model.to(device)  # 모델을 적절한 장치로 이동\n",
        "\n",
        "# 손실 함수와 옵티마이저 정의\n",
        "criterion = nn.CrossEntropyLoss()                # 분류를 위한 교차 엔트로피 손실\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)  # AdamW 옵티마이저\n",
        "\n",
        "# 여러 에포크에 대한 훈련 루프\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()                  # 모델을 훈련 모드로 설정\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    # 현재 에포크에 대한 진행률 표시줄 초기화\n",
        "    progress_bar = tqdm(train_loader, desc=f\"에포크 {epoch+1}/{num_epochs}\")\n",
        "    for batch in progress_bar:\n",
        "        batch_embeddings, batch_labels = batch\n",
        "        batch_embeddings = batch_embeddings.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "        optimizer.zero_grad()      # 역전파 전 그레이디언트 재설정\n",
        "        outputs = model(batch_embeddings)   # 모델을 통한 포워드 패스\n",
        "        loss = criterion(outputs, batch_labels)  # 손실 계산\n",
        "        loss.backward()            # 역전파\n",
        "        optimizer.step()           # 모델 매개변수 업데이트\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # 현재 평균 손실로 진행률 표시줄 업데이트\n",
        "        progress_bar.set_postfix({\"손실\": total_loss / num_batches})\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "\n",
        "    # 각 에포크 후 테스트 세트에서 모델 정확도 평가\n",
        "    test_acc = calculate_accuracy(model, test_loader, device)\n",
        "    print(f\"에포크 [{epoch+1}/{num_epochs}], 테스트 정확도: {test_acc:.4f}\")\n",
        "\n",
        "# 지정된 접두사로 훈련된 모델 디스크에 저장\n",
        "model_name = \"CNN_classifier\"\n",
        "save_model(model, model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRKwcbFlUOpa",
        "outputId": "d33b940a-5264-4937-9cb2-8f95db103999"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용 중인 장치: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "다운로드 중: 100%|██████████| 1.53G/1.53G [01:39<00:00, 16.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "다운로드 완료: vectors.dat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "단어 벡터 로딩 중: 100%|██████████| 3000000/3000000 [00:53<00:00, 55980.14it/s]\n",
            "에포크 1/2: 100%|██████████| 563/563 [00:07<00:00, 76.18it/s, 손실=0.761]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에포크 [1/2], 테스트 정확도: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "에포크 2/2: 100%|██████████| 563/563 [00:05<00:00, 94.79it/s, 손실=0.253]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에포크 [2/2], 테스트 정확도: 0.8995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 지정된 모델 이름 접두사를 사용하여 이전에 저장된 모델을 로드합니다.\n",
        "# 'load_model' 함수는 체크포인트를 읽고 CNNTextClassifier를 재구성합니다.\n",
        "loaded_model = load_model(model_name)\n",
        "\n",
        "# 제공된 URL에서 단어 임베딩을 로드합니다.\n",
        "# 이 함수는 단어에서 임베딩 벡터로의 매핑 사전과 임베딩 차원을 반환합니다.\n",
        "embeddings, emb_dim = load_embeddings(vectors_url)\n",
        "\n",
        "# 토크나이저를 초기화합니다.\n",
        "# 여기서 Tokenizer는 공백을 기준으로 텍스트를 토큰으로 분할하는 간단한 역할을 합니다.\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# 로드된 임베딩, 임베딩 차원 및 고정 시퀀스 길이를 사용하여 Embedder 인스턴스를 생성합니다.\n",
        "# Embedder는 토큰화된 텍스트를 모델에 적합한 임베딩 텐서로 변환합니다.\n",
        "embedder = Embedder(embeddings, emb_dim, seq_len)\n",
        "\n",
        "# 분류할 샘플 입력 텍스트를 정의합니다.\n",
        "test_input = \"I'm so happy to be able to train a text classifier!\"\n",
        "\n",
        "# test_model 함수를 사용하여 샘플 입력에서 로드된 모델을 평가합니다.\n",
        "# 이 함수는 텍스트를 토큰화하고, 임베딩으로 변환하며, 모델을 통한 포워드 패스를 수행하고,\n",
        "# 예측된 레이블을 출력합니다.\n",
        "test_model(loaded_model, test_input, tokenizer, embedder)"
      ],
      "metadata": {
        "id": "CXhorMSZSkdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa3a515-39a7-41f4-c9fb-8ea218ae0b6d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "파일 vectors.dat이 이미 존재합니다. 다운로드를 건너뜁니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "단어 벡터 로딩 중: 100%|██████████| 3000000/3000000 [00:53<00:00, 55747.19it/s]\n",
            "/tmp/ipython-input-2742773537.py:468: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embeddings = torch.tensor(embeddings, dtype=torch.float32).unsqueeze(0).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력: I'm so happy to be able to train a text classifier!\n",
            "예측된 감정: joy\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}