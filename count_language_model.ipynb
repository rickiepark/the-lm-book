{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickiepark/the-lm-book/blob/main/count_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"display: flex; justify-content: center;\">\n",
        "    <div style=\"background-color: #f4f6f7; padding: 15px; width: 80%;\">\n",
        "        <table style=\"width: 100%\">\n",
        "            <tbody><tr>\n",
        "                <td style=\"vertical-align: middle;\">\n",
        "                    <span style=\"font-size: 14px;\">\n",
        "                        A notebook for <a rel=\"noopener\" href=\"https://www.thelmbook.com\">The Hundred-Page Language Models Book</a> by Andriy Burkov<br><br>\n",
        "                        Code repository: <a rel=\"noopener\" href=\"https://github.com/aburkov/theLMbook\">https://github.com/aburkov/theLMbook</a>\n",
        "                    </span>\n",
        "                </td>\n",
        "                <td style=\"vertical-align: middle;\">\n",
        "                    <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">\n",
        "                        <img src=\"https://thelmbook.com/img/book.png\" width=\"80px\" alt=\"The Hundred-Page Language Models Book\">\n",
        "                    </a>\n",
        "                </td>\n",
        "            </tr>\n",
        "        </tbody></table>\n",
        "    </div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "IdYAR6C2_wUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 카운트 기반 언어 모델\n",
        "\n",
        "## 유틸리티 함수와 클래스\n",
        "\n",
        "다음 셀에서 필요 라이브러리를 임포트하고 유틸리티 함수와 모델 클래스를 정의합니다."
      ],
      "metadata": {
        "id": "4R2DpglXC7R4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필수 라이브러리 임포트\n",
        "import re         # 정규 표현식을 위한 라이브러리 (텍스트 토큰화)\n",
        "import requests   # 말뭉치 다운로드를 위한 라이브러리\n",
        "import gzip       # 다운로드한 말뭉치 압축 해제를 위한 라이브러리\n",
        "import io         # 바이트 스트림 처리를 위한 라이브러리\n",
        "import math       # 수학 연산을 위한 라이브러리 (로그, 지수)\n",
        "import random     # 난수 생성을 위한 라이브러리\n",
        "from collections import defaultdict  # 효율적인 딕셔너리 연산을 위한 라이브러리\n",
        "import pickle, os # 모델 저장 및 로드를 위한 라이브러리\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    재현성을 위해 난수 시드를 설정합니다.\n",
        "    Args:\n",
        "        seed (int): 난수 생성기에 사용할 시드 값\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "\n",
        "def download_corpus(url):\n",
        "    \"\"\"\n",
        "    주어진 URL에서 gzip으로 압축된 말뭉치 파일을 다운로드하고 압축을 해제합니다.\n",
        "    Args:\n",
        "        url (str): gzip으로 압축된 말뭉치 파일의 URL\n",
        "    Returns:\n",
        "        str: 말뭉치의 디코딩된 텍스트 내용\n",
        "    Raises:\n",
        "        HTTPError: 다운로드 실패 시 발생하는 예외\n",
        "    \"\"\"\n",
        "    print(f\"{url}에서 말뭉치 다운로드 중...\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # 잘못된 HTTP 응답에 대해 예외를 발생시킴\n",
        "    print(\"말뭉치 압축 해제 및 읽는 중...\")\n",
        "    with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as f:\n",
        "        corpus = f.read().decode('utf-8')\n",
        "    print(f\"말뭉치 크기: {len(corpus)} 문자\")\n",
        "    return corpus\n",
        "\n",
        "class CountLanguageModel:\n",
        "    \"\"\"\n",
        "    카운트 기반 확률 추정을 사용하는 n-그램 언어 모델을 구현합니다.\n",
        "    n-그램까지 가변적인 문맥 길이를 지원합니다.\n",
        "    \"\"\"\n",
        "    def __init__(self, n):\n",
        "        \"\"\"\n",
        "        최대 n-그램 길이로 모델을 초기화합니다.\n",
        "        Args:\n",
        "            n (int): 사용할 n-그램의 최대 길이\n",
        "        \"\"\"\n",
        "        self.n = n  # 최대 n-그램 길이\n",
        "        self.ngram_counts = [{} for _ in range(n)]  # 각 n-그램 길이에 대한 딕셔너리 목록\n",
        "        self.total_unigrams = 0  # 학습 데이터의 총 토큰 수\n",
        "\n",
        "    def predict_next_token(self, context):\n",
        "        \"\"\"\n",
        "        주어진 문맥에서 가장 가능성 있는 다음 토큰을 예측합니다.\n",
        "        백오프 전략 사용: 가장 큰 n-그램부터 시도한 후 더 작은 n-그램으로 백오프합니다.\n",
        "        Args:\n",
        "            context (list): 예측을 위한 문맥을 제공하는 토큰 목록\n",
        "        Returns:\n",
        "            str: 가장 가능성 있는 다음 토큰, 예측을 할 수 없는 경우 None\n",
        "        \"\"\"\n",
        "        for n in range(self.n, 1, -1):  # 가장 큰 n-그램으로 시작하여 더 작은 것으로 백오프\n",
        "            if len(context) >= n - 1:\n",
        "                context_n = tuple(context[-(n - 1):])  # 이 n-그램에 대한 관련 문맥 가져오기\n",
        "                counts = self.ngram_counts[n - 1].get(context_n)\n",
        "                if counts:\n",
        "                    return max(counts.items(), key=lambda x: x[1])[0]  # 가장 빈번한 토큰 반환\n",
        "        # 더 큰 문맥이 일치하지 않는 경우 유니그램으로 백오프\n",
        "        unigram_counts = self.ngram_counts[0].get(())\n",
        "        if unigram_counts:\n",
        "            return max(unigram_counts.items(), key=lambda x: x[1])[0]\n",
        "        return None\n",
        "\n",
        "    def get_probability(self, token, context):\n",
        "        for n in range(self.n, 1, -1):\n",
        "            if len(context) >= n - 1:\n",
        "                context_n = tuple(context[-(n - 1):])\n",
        "                counts = self.ngram_counts[n - 1].get(context_n)\n",
        "                if counts:\n",
        "                    total = sum(counts.values())\n",
        "                    count = counts.get(token, 0)\n",
        "                    if count > 0:\n",
        "                        return count / total\n",
        "        unigram_counts = self.ngram_counts[0].get(())\n",
        "        count = unigram_counts.get(token, 0)\n",
        "        V = len(unigram_counts)\n",
        "        return (count + 1) / (self.total_unigrams + V)\n",
        "\n",
        "def train(model, tokens):\n",
        "    \"\"\"\n",
        "    학습 데이터에서 n-그램을 계수하여 언어 모델을 학습시킵니다.\n",
        "    Args:\n",
        "        model (CountLanguageModel): 학습할 모델\n",
        "        tokens (list): 학습 말뭉치의 토큰 목록\n",
        "    \"\"\"\n",
        "    # 1부터 n까지 각 n-그램 크기에 대한 모델 학습\n",
        "    for n in range(1, model.n + 1):\n",
        "        counts = model.ngram_counts[n - 1]\n",
        "        # 말뭉치 위에 크기 n의 창을 슬라이드\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            # 문맥(n-1 토큰)과 다음 토큰으로 분할\n",
        "            context = tuple(tokens[i:i + n - 1])\n",
        "            next_token = tokens[i + n - 1]\n",
        "            # 필요한 경우 이 문맥에 대한 카운트 딕셔너리 초기화\n",
        "            if context not in counts:\n",
        "                counts[context] = defaultdict(int)\n",
        "            # 이 문맥-토큰 쌍의 카운트 증가\n",
        "            counts[context][next_token] = counts[context][next_token] + 1\n",
        "    # 유니그램 확률 계산을 위한 총 토큰 수 저장\n",
        "    model.total_unigrams = len(tokens)\n",
        "\n",
        "def generate_text(model, context, num_tokens):\n",
        "    \"\"\"\n",
        "    모델에서 반복적으로 샘플링하여 텍스트를 생성합니다.\n",
        "    Args:\n",
        "        model (CountLanguageModel): 학습된 언어 모델\n",
        "        context (list): 초기 문맥 토큰\n",
        "        num_tokens (int): 생성할 토큰 수\n",
        "    Returns:\n",
        "        str: 초기 문맥을 포함한 생성된 텍스트\n",
        "    \"\"\"\n",
        "    # 제공된 문맥으로 시작\n",
        "    generated = list(context)\n",
        "    # 원하는 길이에 도달할 때까지 새 토큰 생성\n",
        "    while len(generated) - len(context) < num_tokens:\n",
        "        # 예측을 위한 문맥으로 마지막 n-1 토큰 사용\n",
        "        next_token = model.predict_next_token(generated[-(model.n-1):])\n",
        "        generated.append(next_token)\n",
        "        # 충분한 토큰을 생성하고 마침표를 찾으면 중지\n",
        "        # 이는 완전한 문장을 보장하는 데 도움이 됨\n",
        "        if len(generated) - len(context) >= num_tokens and next_token == '.':\n",
        "            break\n",
        "    # 읽을 수 있는 텍스트를 만들기 위해 토큰을 공백으로 연결\n",
        "    return ' '.join(generated)\n",
        "\n",
        "def compute_perplexity(model, tokens, context_size):\n",
        "    \"\"\"\n",
        "    주어진 토큰에 대한 모델의 혼잡도를 계산합니다.\n",
        "    Args:\n",
        "        model (CountLanguageModel): 학습된 언어 모델\n",
        "        tokens (list): 평가할 토큰 목록\n",
        "        context_size (int): 고려할 최대 문맥 크기\n",
        "    Returns:\n",
        "        float: 혼잡도 점수 (낮을수록 좋음)\n",
        "    \"\"\"\n",
        "    # 빈 토큰 목록 처리\n",
        "    if not tokens:\n",
        "        return float('inf')\n",
        "    # 로그 우도 누적 변수 초기화\n",
        "    total_log_likelihood = 0\n",
        "    num_tokens = len(tokens)\n",
        "    # 각 토큰의 문맥에 주어진 확률 계산\n",
        "    for i in range(num_tokens):\n",
        "        # 적절한 문맥 윈도 가져오기, 시퀀스 시작 처리\n",
        "        context_start = max(0, i - context_size)\n",
        "        context = tuple(tokens[context_start:i])\n",
        "        token = tokens[i]\n",
        "        # 주어진 문맥에 대한 토큰의 확률 가져오기\n",
        "        probability = model.get_probability(token, context)\n",
        "        # 로그 확률 누적 (수치적 안정성을 위해 로그 사용)\n",
        "        total_log_likelihood += math.log(probability)\n",
        "    # 평균 로그 우도 계산\n",
        "    average_log_likelihood = total_log_likelihood / num_tokens\n",
        "    # 혼잡도로 변환: exp(-평균 로그 우도)\n",
        "    # 낮은 혼잡도는 더 나은 모델 성능을 나타냄\n",
        "    perplexity = math.exp(-average_log_likelihood)\n",
        "    return perplexity\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    텍스트를 단어와 마침표로 토큰화합니다.\n",
        "    Args:\n",
        "        text (str): 토큰화할 입력 텍스트\n",
        "    Returns:\n",
        "        list: 단어나 마침표와 일치하는 소문자 토큰 목록\n",
        "    \"\"\"\n",
        "    return re.findall(r\"\\b[a-zA-Z0-9]+\\b|[.]\", text.lower())\n",
        "\n",
        "def download_and_prepare_data(data_url):\n",
        "    \"\"\"\n",
        "    훈련 및 테스트 데이터를 다운로드하고 준비합니다.\n",
        "    Args:\n",
        "        data_url (str): 다운로드할 말뭉치의 URL\n",
        "    Returns:\n",
        "        tuple: (훈련_토큰, 테스트_토큰) 90/10으로 분할\n",
        "    \"\"\"\n",
        "    # 말뭉치 다운로드 및 추출\n",
        "    corpus = download_corpus(data_url)\n",
        "    # 텍스트를 토큰으로 변환\n",
        "    tokens = tokenize(corpus)\n",
        "    # 훈련(90%) 및 테스트(10%) 세트로 분할\n",
        "    split_index = int(len(tokens) * 0.9)\n",
        "    train_corpus = tokens[:split_index]\n",
        "    test_corpus = tokens[split_index:]\n",
        "    return train_corpus, test_corpus\n",
        "\n",
        "def save_model(model, model_name):\n",
        "    \"\"\"\n",
        "    훈련된 언어 모델을 디스크에 저장합니다.\n",
        "    Args:\n",
        "        model (CountLanguageModel): 저장할 훈련 모델\n",
        "        model_name (str): 저장된 모델 파일에 사용할 이름\n",
        "    Returns:\n",
        "        str: 저장된 모델 파일의 경로\n",
        "    Raises:\n",
        "        IOError: 디스크 쓰기 오류가 있는 경우\n",
        "    \"\"\"\n",
        "    # 모델 디렉토리가 없는 경우 생성\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    # 파일 경로 구성\n",
        "    model_path = os.path.join('models', f'{model_name}.pkl')\n",
        "    try:\n",
        "        print(f\"{model_path}에 모델 저장 중...\")\n",
        "        with open(model_path, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'n': model.n,\n",
        "                'ngram_counts': model.ngram_counts,\n",
        "                'total_unigrams': model.total_unigrams\n",
        "            }, f)\n",
        "        print(\"모델이 성공적으로 저장되었습니다.\")\n",
        "        return model_path\n",
        "    except IOError as e:\n",
        "        print(f\"모델 저장 오류: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_model(model_name):\n",
        "    \"\"\"\n",
        "    디스크에서 훈련된 언어 모델을 로드합니다.\n",
        "    Args:\n",
        "        model_name (str): 로드할 모델의 이름\n",
        "    Returns:\n",
        "        CountLanguageModel: 로드된 모델 인스턴스\n",
        "    Raises:\n",
        "        FileNotFoundError: 모델 파일이 존재하지 않는 경우\n",
        "        IOError: 파일 읽기 오류가 있는 경우\n",
        "    \"\"\"\n",
        "    model_path = os.path.join('models', f'{model_name}.pkl')\n",
        "    try:\n",
        "        print(f\"{model_path}에서 모델 로드 중...\")\n",
        "        with open(model_path, 'rb') as f:\n",
        "            model_data = pickle.load(f)\n",
        "        # 새 모델 인스턴스 생성\n",
        "        model = CountLanguageModel(model_data['n'])\n",
        "        # 모델 상태 복원\n",
        "        model.ngram_counts = model_data['ngram_counts']\n",
        "        model.total_unigrams = model_data['total_unigrams']\n",
        "        print(\"모델이 성공적으로 로드되었습니다.\")\n",
        "        return model\n",
        "    except FileNotFoundError:\n",
        "        print(f\"모델 파일을 찾을 수 없음: {model_path}\")\n",
        "        raise\n",
        "    except IOError as e:\n",
        "        print(f\"모델 로드 오류: {e}\")\n",
        "        raise\n",
        "\n",
        "def get_hyperparameters():\n",
        "    \"\"\"\n",
        "    모델 하이퍼파라미터를 반환합니다.\n",
        "    Returns:\n",
        "        int: 모델에서 사용할 n-그램 크기\n",
        "    \"\"\"\n",
        "    n = 5\n",
        "    return n"
      ],
      "metadata": {
        "id": "DDVuiMnmRHaJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 훈련하기\n",
        "\n",
        "다음 셀에서 데이터를 로드하고, 모델을 훈련 및 저장합니다."
      ],
      "metadata": {
        "id": "ug7YhOF6Dczx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 재현성을 위해 랜덤 시드를 설정합니다.\n",
        "set_seed(42)\n",
        "n = get_hyperparameters()\n",
        "model_name = \"count_model\"\n",
        "\n",
        "# Brown 말뭉치를 다운로드하여 준비합니다.\n",
        "data_url = \"https://www.thelmbook.com/data/brown\"\n",
        "train_corpus, test_corpus = download_and_prepare_data(data_url)\n",
        "\n",
        "# 모델을 훈련하고 성능을 평가합니다.\n",
        "print(\"\\n모델 훈련 중...\")\n",
        "model = CountLanguageModel(n)\n",
        "train(model, train_corpus)\n",
        "print(\"\\n모델 훈련 완료.\")\n",
        "\n",
        "perplexity = compute_perplexity(model, test_corpus, n)\n",
        "print(f\"\\n테스트 말뭉치에 대한 혼잡도: {perplexity:.2f}\")\n",
        "\n",
        "save_model(model, model_name)"
      ],
      "metadata": {
        "id": "l1z9hxbJDmm0",
        "outputId": "75c79a95-8538-4a9f-fbac-7fa570aa31c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.thelmbook.com/data/brown에서 말뭉치 다운로드 중...\n",
            "말뭉치 압축 해제 및 읽는 중...\n",
            "말뭉치 크기: 6185606 문자\n",
            "\n",
            "모델 훈련 중...\n",
            "\n",
            "모델 훈련 완료.\n",
            "\n",
            "테스트 말뭉치에 대한 혼잡도: 299.06\n",
            "models/count_model.pkl에 모델 저장 중...\n",
            "모델이 성공적으로 저장되었습니다.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'models/count_model.pkl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 테스트하기\n",
        "\n",
        "아래 셀에서 훈련된 모델을 로드하여 텍스트를 생성합니다."
      ],
      "metadata": {
        "id": "Glud7JWgDyMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name)\n",
        "\n",
        "# 샘플 문맥으로 모델을 테스트합니다.\n",
        "contexts = [\n",
        "    \"i will build a\",\n",
        "    \"the best place to\",\n",
        "    \"she was riding a\"\n",
        "]\n",
        "\n",
        "# 각 문맥에 대해 완성을 생성합니다.\n",
        "for context in contexts:\n",
        "    tokens = tokenize(context)\n",
        "    next_token = model.predict_next_token(tokens)\n",
        "    print(f\"\\n문맥: {context}\")\n",
        "    print(f\"다음 토큰: {next_token}\")\n",
        "    print(f\"생성된 텍스트: {generate_text(model, tokens, 10)}\")"
      ],
      "metadata": {
        "id": "tesX-DH9D23a",
        "outputId": "fe1726d2-ecdd-46ec-8429-e02790d3fc13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/count_model.pkl에서 모델 로드 중...\n",
            "모델이 성공적으로 로드되었습니다.\n",
            "\n",
            "문맥: i will build a\n",
            "다음 토큰: wall\n",
            "생성된 텍스트: i will build a wall to keep the people in and added so long\n",
            "\n",
            "문맥: the best place to\n",
            "다음 토큰: live\n",
            "생성된 텍스트: the best place to live in 30 per cent to get happiness for yourself\n",
            "\n",
            "문맥: she was riding a\n",
            "다음 토큰: horse\n",
            "생성된 텍스트: she was riding a horse and showing a dog are very similar your aids\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}