{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickiepark/the-lm-book/blob/main/embedding_vs_linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c08a8825-1a0b-41e9-9c9d-bab840d333a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c08a8825-1a0b-41e9-9c9d-bab840d333a2",
        "outputId": "b0e881e5-f9e0-4235-f3aa-7bc5d5dbbb32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 출력:\n",
            "tensor([[ 0.7639,  0.8223, -0.8133, -0.2121]], grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "Linear 출력:\n",
            "tensor([[ 0.7639,  0.8223, -0.8133, -0.2121]], grad_fn=<MmBackward0>)\n",
            "\n",
            "텐서가 동일한가요? True\n",
            "텐서가 유사한가요? True\n",
            "\n",
            "텐서 간 최대 차이: 0.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "vocab_size = 10    # 시각화를 위해 작은 어휘 크기 사용\n",
        "emb_dim = 4       # 작은 임베딩 차원\n",
        "token_idx = 3     # 임베딩할 토큰의 인덱스\n",
        "\n",
        "# 접근 방식 1: nn.Embedding 사용하여 인덱스를 밀집 벡터에 직접 매핑\n",
        "embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "# 접근 방식 2: 동일한 효과를 얻기 위해 nn.Linear 사용\n",
        "# Linear 층은 다음을 수행합니다: output = input @ weight.t() + bias\n",
        "# 이 경우 bias=False 이므로: output = input @ weight.t()\n",
        "linear = nn.Linear(vocab_size, emb_dim, bias=False)\n",
        "\n",
        "# 임베딩 가중치를 Linear 층에 복사하고 전치\n",
        "# 임베딩 가중치 크기: (vocab_size, emb_dim)\n",
        "# Linear 가중치 크기: (emb_dim, vocab_size)  <- 전치에 유의\n",
        "linear.weight.data = embedding.weight.data.t()\n",
        "\n",
        "# Linear 층을 위한 원-핫 입력 생성 - token_idx를 제외한 모든 곳이 0\n",
        "one_hot = torch.zeros(vocab_size)\n",
        "one_hot[token_idx] = 1\n",
        "\n",
        "# 임베딩 행렬에서 직접 임베딩 가져오기\n",
        "# 내부적으로 embedding.weight[token_idx]가 수행됨\n",
        "emb_output = embedding(torch.tensor([token_idx]))\n",
        "\n",
        "# Linear 층의 경우: Linear 층은 (batch_size, input_dim) 형태를 예상하므로 배치 차원 추가\n",
        "# 결과는: one_hot @ weight.t()이며, 이는 weight.t()에서 token_idx 행을 선택합니다.\n",
        "linear_output = linear(one_hot.unsqueeze(0))\n",
        "\n",
        "# 동일성을 확인하기 위해 출력 및 비교 결과 출력\n",
        "print(f\"임베딩 출력:\\n{emb_output}\\n\")\n",
        "print(f\"Linear 출력:\\n{linear_output}\\n\")\n",
        "print(f\"텐서가 동일한가요? {torch.equal(emb_output, linear_output)}\")\n",
        "print(f\"텐서가 유사한가요? {torch.allclose(emb_output, linear_output)}\")\n",
        "\n",
        "# 숫자 정밀도 내에서 출력이 동일한지 확인\n",
        "# 기본 허용 오차 사용: rtol=1e-5, atol=1e-8\n",
        "# 공식: |x - y| ≤ atol + rtol * |y|\n",
        "print(f\"\\n텐서 간 최대 차이: {(emb_output - linear_output).abs().max().item()}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}