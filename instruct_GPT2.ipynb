{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rickiepark/the-lm-book/blob/main/instruct_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu6Fr-_WuSMZ"
   },
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <div style=\"background-color: #f4f6f7; padding: 15px; width: 80%;\">\n",
    "        <table style=\"width: 100%\">\n",
    "            <tr>\n",
    "                <td style=\"vertical-align: middle;\">\n",
    "                    <span style=\"font-size: 14px;\">\n",
    "                        <a href=\"https://tensorflow.blog/the-lm-book\" target=\"_blank\" rel=\"noopener\"><대규모 언어 모델, 핵심만 빠르게!>(인사이트, 2025)</a>의 주피터 노트북<br><br>\n",
    "                        코드 저장소: <a href=\"https://github.com/rickiepark/the-lm-book\" target=\"_blank\" rel=\"noopener\">https://github.com/rickiepark/the-lm-book</a>\n",
    "                    </span>\n",
    "                </td>\n",
    "                <td style=\"vertical-align: middle;\">\n",
    "                    <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">\n",
    "                        <img src=\"https://tensorflow.blog/wp-content/uploads/2025/10/cover-the-lm-book.jpg\" width=\"80px\" alt=\"대규모 언어 모델, 핵심만 빠르게!\" border=\"1\">\n",
    "                    </a>\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yy0zjL_2ouOU"
   },
   "outputs": [],
   "source": [
    "# 필수 라이브러리 임포트\n",
    "import json            # JSON 데이터 구문 분석을 위한 라이브러리\n",
    "import random          # 시드 설정 및 데이터 셔플링을 위한 라이브러리\n",
    "import requests        # URL에서 데이터셋 다운로드를 위한 라이브러리\n",
    "import torch           # 메인 파이토치 라이브러리\n",
    "from torch.utils.data import Dataset, DataLoader  # 데이터셋 처리를 위한 라이브러리\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria  # 허깅 페이스 구성 요소\n",
    "from tqdm import tqdm   # 진행률 표시줄 유틸리티\n",
    "import re               # 텍스트 정규화를 위한 라이브러리\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    다양한 라이브러리에서 재현성을 위해 난수 시드를 설정합니다.\n",
    "    Args:\n",
    "        seed (int): 난수 생성을 위한 시드 값\n",
    "    \"\"\"\n",
    "    # 파이썬 내장 random 시드 설정\n",
    "    random.seed(seed)\n",
    "    # 파이토치 CPU 난수 시드 설정\n",
    "    torch.manual_seed(seed)\n",
    "    # 사용 가능한 모든 GPU에 대한 시드 설정\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # cuDNN이 결정론적 알고리즘을 사용하도록 요청\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # 일관된 동작을 위해 cuDNN의 자동 튜너 비활성화\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def build_prompt(instruction, solution=None):\n",
    "    \"\"\"\n",
    "    시스템, 사용자 및 어시스턴트 메시지가 있는 채팅 형식의 프롬프트를 생성합니다.\n",
    "    Args:\n",
    "        instruction (str): 사용자의 지시/질문\n",
    "        solution (str, optional): 훈련을 위한 예상 응답\n",
    "    Returns:\n",
    "        str: 형식화된 프롬프트 문자열\n",
    "    \"\"\"\n",
    "    # 제공된 경우 종료 토큰과 함께 솔루션 추가\n",
    "    wrapped_solution = \"\"\n",
    "    if solution:\n",
    "        wrapped_solution = f\"\\n{solution}\\n<|im_end|>\"\n",
    "    # 시스템, 사용자 및 어시스턴트 메시지로 채팅 형식 구축\n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are a helpful assistant.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\" + wrapped_solution\n",
    "\n",
    "def encode_text(tokenizer, text, return_tensor=False):\n",
    "    \"\"\"\n",
    "    제공된 토크나이저를 사용하여 텍스트를 인코딩합니다.\n",
    "    Args:\n",
    "        tokenizer: 허깅 페이스 토크나이저\n",
    "        text (str): 인코딩할 텍스트\n",
    "        return_tensor (bool): 파이토치 텐서를 반환할지 여부\n",
    "    Returns:\n",
    "        토큰 ID의 목록 또는 텐서\n",
    "    \"\"\"\n",
    "    # 텐서 출력이 요청된 경우, 파이토치 텐서로 인코딩\n",
    "    if return_tensor:\n",
    "        return tokenizer.encode(\n",
    "            text, add_special_tokens=False, return_tensors=\"pt\"\n",
    "        )\n",
    "    # 그렇지 않으면 토큰 ID 목록 반환\n",
    "    else:\n",
    "        return tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "class EndTokenStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"\n",
    "    텍스트 생성을 위한 사용자 정의 중단 기준.\n",
    "    특정 종료 토큰 시퀀스가 생성되면 중지합니다.\n",
    "    Args:\n",
    "        end_tokens (list): 생성이 중지되어야 함을 나타내는 토큰 ID\n",
    "        device: 모델이 실행 중인 장치\n",
    "    \"\"\"\n",
    "    def __init__(self, end_tokens, device):\n",
    "        self.end_tokens = torch.tensor(end_tokens).to(device)\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        \"\"\"\n",
    "        각 시퀀스에 대해 생성을 중지해야 하는지 확인합니다.\n",
    "        Args:\n",
    "            input_ids: 현재 생성된 토큰 ID\n",
    "            scores: 토큰 확률\n",
    "        Returns:\n",
    "            tensor: 어떤 시퀀스를 중지해야 하는지 나타내는 부울 텐서\n",
    "        \"\"\"\n",
    "        should_stop = []\n",
    "        # 종료 토큰에 대해 각 시퀀스 확인\n",
    "        for sequence in input_ids:\n",
    "            if len(sequence) >= len(self.end_tokens):\n",
    "                # 마지막 토큰을 종료 토큰과 비교\n",
    "                last_tokens = sequence[-len(self.end_tokens):]\n",
    "                should_stop.append(torch.all(last_tokens == self.end_tokens))\n",
    "            else:\n",
    "                should_stop.append(False)\n",
    "        return torch.tensor(should_stop, device=input_ids.device)\n",
    "\n",
    "class PromptCompletionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    지시-완성 쌍을 위한 파이토치 데이터셋.\n",
    "    텍스트 데이터를 모델 준비 형식으로 변환하는 작업을 처리합니다.\n",
    "    Args:\n",
    "        data (list): 지시와 솔루션을 포함하는 사전 목록\n",
    "        tokenizer: 허깅 페이스 토크나이저\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        # 총 샘플 수 반환\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        단일 훈련 샘플을 반환합니다.\n",
    "        Args:\n",
    "            idx (int): 가져올 샘플의 인덱스\n",
    "        Returns:\n",
    "            dict: input_ids, labels, prompt, expected_completion을 포함\n",
    "        \"\"\"\n",
    "        # 데이터셋에서 샘플 가져오기\n",
    "        item = self.data[idx]\n",
    "        # 지시와 함께 전체 프롬프트 구축\n",
    "        prompt = build_prompt(item[\"instruction\"])\n",
    "        # 종료 토큰으로 완성 형식 지정\n",
    "        completion = f\"\"\"{item[\"solution\"]}\\n<|im_end|>\"\"\"\n",
    "        # 텍스트를 토큰 ID로 변환\n",
    "        encoded_prompt = encode_text(self.tokenizer, prompt)\n",
    "        encoded_completion = encode_text(self.tokenizer, completion)\n",
    "        eos_token = [self.tokenizer.eos_token_id]\n",
    "        # 전체 입력 시퀀스를 위해 결합\n",
    "        input_ids = encoded_prompt + encoded_completion + eos_token\n",
    "        # 레이블 생성: 프롬프트의 경우 -100(손실에서 무시됨)\n",
    "        labels = [-100] * len(encoded_prompt) + encoded_completion + eos_token\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"prompt\": prompt,\n",
    "            \"expected_completion\": completion\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    샘플 배치를 훈련 준비 형식으로 통합합니다.\n",
    "    패딩과 텐서 변환을 처리합니다.\n",
    "    Args:\n",
    "        batch: 데이터셋의 샘플 목록\n",
    "    Returns:\n",
    "        tuple: (input_ids, attention_mask, labels, prompts, expected_completions)\n",
    "    \"\"\"\n",
    "    # 패딩을 위해 가장 긴 시퀀스 찾기\n",
    "    max_length = max(len(item[\"input_ids\"]) for item in batch)\n",
    "    # 입력 시퀀스 패딩\n",
    "    input_ids = [\n",
    "        item[\"input_ids\"] +\n",
    "        [tokenizer.pad_token_id] * (max_length - len(item[\"input_ids\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "    # 레이블 시퀀스 패딩\n",
    "    labels = [\n",
    "        item[\"labels\"] +\n",
    "        [-100] * (max_length - len(item[\"labels\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "    # 어텐션 마스크 생성\n",
    "    attention_mask = [\n",
    "        [1] * len(item[\"input_ids\"]) +\n",
    "        [0] * (max_length - len(item[\"input_ids\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "    prompts = [item[\"prompt\"] for item in batch]\n",
    "    expected_completions = [item[\"expected_completion\"] for item in batch]\n",
    "    return (\n",
    "        torch.tensor(input_ids),\n",
    "        torch.tensor(attention_mask),\n",
    "        torch.tensor(labels),\n",
    "        prompts,\n",
    "        expected_completions\n",
    "    )\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    일관된 비교를 위해 텍스트를 정규화합니다.\n",
    "    Args:\n",
    "        text (str): 입력 텍스트\n",
    "    Returns:\n",
    "        str: 정규화된 텍스트\n",
    "    \"\"\"\n",
    "    # 선행/후행 공백 제거 및 소문자로 변환\n",
    "    text = text.strip().lower()\n",
    "    # 여러 공백 문자를 단일 공백으로 대체\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    주어진 프롬프트에 대한 텍스트 완성을 생성합니다.\n",
    "    Args:\n",
    "        model: 미세 튜닝된 모델\n",
    "        tokenizer: 연관된 토크나이저\n",
    "        prompt (str): 입력 프롬프트\n",
    "        max_new_tokens (int): 생성할 최대 토큰 수\n",
    "    Returns:\n",
    "        str: 생성된 완성\n",
    "    \"\"\"\n",
    "    # 프롬프트 인코딩 및 모델 장치로 이동\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    # 종료 토큰 감지 설정\n",
    "    end_tokens = tokenizer.encode(\"<|im_end|>\", add_special_tokens=False)\n",
    "    stopping_criteria = [EndTokenStoppingCriteria(end_tokens, model.device)]\n",
    "    # 완성 생성\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids[\"input_ids\"],\n",
    "        attention_mask=input_ids[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        stopping_criteria=stopping_criteria\n",
    "    )[0]\n",
    "    # 생성된 부분만 추출 및 디코딩\n",
    "    generated_ids = output_ids[input_ids[\"input_ids\"].shape[1]:]\n",
    "    generated_text = tokenizer.decode(generated_ids).strip()\n",
    "    return generated_text\n",
    "\n",
    "def test_model(model_path, test_input):\n",
    "    \"\"\"\n",
    "    저장된 모델을 단일 입력으로 테스트합니다.\n",
    "    Args:\n",
    "        model_path (str): 저장된 모델의 경로\n",
    "        test_input (str): 테스트할 지시\n",
    "    \"\"\"\n",
    "    # 장치 설정 및 모델 로드\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"사용 중인 장치: {device}\")\n",
    "    # 모델과 토크나이저 로드\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # 예측 생성 및 표시\n",
    "    prompt = build_prompt(test_input)\n",
    "    generated_text = generate_text(model, tokenizer, prompt)\n",
    "    print(f\"\\n입력: {test_input}\")\n",
    "    print(f\"전체 생성된 텍스트: {generated_text}\")\n",
    "    print(f\"\"\"응답: {generated_text.replace(\"<|im_end|>\", \"\").strip()}\"\"\")\n",
    "\n",
    "def download_and_prepare_data(data_url, tokenizer, batch_size, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    훈련을 위해 데이터셋을 다운로드하고 준비합니다.\n",
    "    Args:\n",
    "        data_url (str): 데이터셋의 URL\n",
    "        tokenizer: 텍스트 처리를 위한 토크나이저\n",
    "        batch_size (int): DataLoader의 배치 크기\n",
    "        test_ratio (float): 테스트를 위한 데이터 비율\n",
    "    Returns:\n",
    "        tuple: (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # 데이터셋 다운로드\n",
    "    response = requests.get(data_url)\n",
    "    dataset = []\n",
    "    # 각 라인을 지시-솔루션 쌍으로 구문 분석\n",
    "    for line in response.text.splitlines():\n",
    "        if line.strip():  # 빈 라인 건너뛰기\n",
    "            entry = json.loads(line)\n",
    "            dataset.append({\n",
    "                \"instruction\": entry[\"instruction\"],\n",
    "                \"solution\": entry[\"solution\"]\n",
    "            })\n",
    "    # 훈련 및 테스트 세트로 분할\n",
    "    random.shuffle(dataset)\n",
    "    split_index = int(len(dataset) * (1 - test_ratio))\n",
    "    train_data = dataset[:split_index]\n",
    "    test_data = dataset[split_index:]\n",
    "    # 데이터셋 통계 출력\n",
    "    print(f\"\\n데이터셋 크기: {len(dataset)}\")\n",
    "    print(f\"훈련 샘플: {len(train_data)}\")\n",
    "    print(f\"테스트 샘플: {len(test_data)}\")\n",
    "    # 데이터셋 생성\n",
    "    train_dataset = PromptCompletionDataset(train_data, tokenizer)\n",
    "    test_dataset = PromptCompletionDataset(test_data, tokenizer)\n",
    "    # 데이터로더 생성\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_hyperparameters():\n",
    "    \"\"\"\n",
    "    훈련 하이퍼파라미터를 반환합니다.\n",
    "    Returns:\n",
    "        tuple: (num_epochs, batch_size, learning_rate)\n",
    "    \"\"\"\n",
    "    # 지시 미세 튜닝은 더 데이터 효율적이므로 더 적은 에포크\n",
    "    num_epochs = 4\n",
    "    # 대부분의 GPU 메모리에서 잘 작동하는 표준 배치 크기\n",
    "    batch_size = 16\n",
    "    # 트랜스포머 미세 튜닝을 위한 표준 학습률\n",
    "    learning_rate = 5e-5\n",
    "    return num_epochs, batch_size, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 681,
     "referenced_widgets": [
      "f65666fd841245e299526258fd7e4de2",
      "7af8520336624535b5312ef9f8b32e05",
      "1d03063489934cc98db44006dec823d6",
      "342e57d3c34e461886f0f93eeae14a6f",
      "dfd18573682b4828b4e9acd0282b8d57",
      "290c7e886c1a4976aeb2e987257f47cd",
      "c2157ac6b1a64cf38f046403510ac2f4",
      "4232dc68859b4b60bc7353c121dba42a",
      "878c82dd2b8e45f39bdf4c8375b120ef",
      "92ea0fc5962f4fd2b340ef8e3289319d",
      "a5394c68d3f849cf86db5c7ef84da519",
      "9e6607b69c584b6693a013a08c1f1a97",
      "9d70ed211d6c4593b6a734620aa79bb8",
      "4d61970e559c463aa39265c29ba9e807",
      "250884fb02fb438096ca637748319460",
      "7a0eed111c7946aeaad1d6035a210111",
      "1a5ca7a5902d4e1fbf49c75079bd4eaa",
      "0dcdbc0906204809997546e9d1ca65ee",
      "1ef92e2f0ee6428ba89fee6e596c4533",
      "d070f7cdc01540ac940901691e16e07d",
      "33f15cf998584cfa80e3ab18861acff6",
      "8ea9673e0db74bfb87dc88f9b8774f5a",
      "c5784c42cda543a9afa4a515ace2e12f",
      "0774d22f18c741b89218f111d064a9a0",
      "c4a4d7aae6bb4397b693f2e96eb3ef2e",
      "61ac06bbcb0e4daca1e4baf1f501e9a7",
      "623ca2571f38426fa304801fa67a58fd",
      "9b01f9e6911e43c0a0ba61c4bb7dae3c",
      "781a42e2eeca4b148ce12eae574e104e",
      "8f0c1fe2b4c54decaa751974bc28a177",
      "86d8d9929829452bb6843a63e2040436",
      "5013464a88174d3487ff5afa1b68e50e",
      "c18cfce15e124481baa5175aa936c9f6",
      "cdea51a11ef74b84bab5708107c06da1",
      "07c65f9ff2ce4ea2aff32c0243c10674",
      "7b98a50a956b410ca77644f292708fae",
      "5afe770980cb4c9da801c035d33939e7",
      "8b33739e235246a0a60f7e049e8e5950",
      "fdc967b3067d4a40aa576c1116033949",
      "ba993e58754e4e8ab6ca46d1d209f2ec",
      "a5663c7c9a0947618322b6cd462aec5e",
      "c0d14175d81a485fa97797bb3450ef70",
      "c09fcb183ba947c1a799ee86dacf305b",
      "ad72b9f132f44dddb217a2c0f9601702",
      "a55ff143106b4eccbeea206df51f16b3",
      "3ff9764761684dc7bf64cde798c03f85",
      "7bfdc62cadf54e2f9bdf663e1785e7a5",
      "c0bdf90761ca46c18fa52f69d6a9baee",
      "4429e1ff5f0c4defbb0ec69d36f0c984",
      "f1e4cce8502246b3b023ec6ffb9df1e8",
      "7eb70e51e2d842d1859056b70877b326",
      "426103a2fe2f47709cecbf08726c66f3",
      "492840267eb54ca297c354fddb6c3e12",
      "65efe40219bd47b386ec0b8d1e6762d3",
      "91eeb053ceb14c28ba0347d10806b832",
      "ea5c6e21345a465a8d37f3f86ba06399",
      "0e7f41d048624ef8b1a892ad1a018684",
      "2d5f90a64c4b4b31bee31a220c5e6f55",
      "20874f3a31554664a098cf5f76683dee",
      "e16ef7a8f519464ab01abdbdbc870972",
      "e11eeda2699b41d1bd030fc5b6561875",
      "8c906ad4039f4b209d2e5e7acf25e231",
      "271db35b93f04d289cdbadafd32e26ee",
      "40a40f3d345741499bf51432ae6c11d9",
      "e3ec67c2ec754ec383fb950eb4867a59",
      "9488be8e3bea4bf6a744c8484da33225",
      "9f16bdf0af02469a9c210d2308c2d2e6",
      "670cc1ee8f5a4849b770f2a56b3fbe11",
      "6d6633f4bc29408da4d8f4e1176cb66a",
      "8f92a09ffb1b4d15950ac8fb69810c3b",
      "0186da1cceaa4000b7ca7883b0ab4ea7",
      "85e8dd593a054c70be315e70c5c68c2b",
      "57326917aa994393a27059267251bb04",
      "62d905f2516847afaf21508847f2b054",
      "88788a91e73c440e8d74b13543fccda0",
      "d508fa4cf0a14e1a9d401f1f9f5b75e4",
      "c1979f7d151340ada60c634d2cfa2403"
     ]
    },
    "id": "EvbVQfxmZMBc",
    "outputId": "5ed5cc57-9441-4a6c-b385-48fa95fbfc75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 장치: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65666fd841245e299526258fd7e4de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6607b69c584b6693a013a08c1f1a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5784c42cda543a9afa4a515ace2e12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdea51a11ef74b84bab5708107c06da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55ff143106b4eccbeea206df51f16b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5c6e21345a465a8d37f3f86ba06399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f16bdf0af02469a9c210d2308c2d2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "데이터셋 크기: 510\n",
      "훈련 샘플: 459\n",
      "테스트 샘플: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 1/4:   0%|          | 0/29 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "에포크 1/4: 100%|██████████| 29/29 [00:04<00:00,  6.20it/s, 손실=1.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 1 - 평균 손실: 1.6233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 2/4: 100%|██████████| 29/29 [00:03<00:00,  7.91it/s, 손실=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 2 - 평균 손실: 1.0221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 3/4: 100%|██████████| 29/29 [00:03<00:00,  7.93it/s, 손실=0.688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 3 - 평균 손실: 0.6885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 4/4: 100%|██████████| 29/29 [00:03<00:00,  7.79it/s, 손실=0.422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 4 - 평균 손실: 0.4221\n",
      "\n",
      "미세 튜닝된 모델 테스트:\n",
      "사용 중인 장치: cuda\n",
      "\n",
      "입력: Who is the President of the United States?\n",
      "전체 생성된 텍스트: George W. Bush\n",
      "<|im_end|>\n",
      "응답: George W. Bush\n"
     ]
    }
   ],
   "source": [
    "# 재현성을 위해 난수 시드 설정\n",
    "set_seed(42)\n",
    "\n",
    "# 훈련 매개변수 구성\n",
    "data_url = \"https://www.thelmbook.com/data/instruct\"\n",
    "model_name = \"openai-community/gpt2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 장치: {device}\")\n",
    "\n",
    "# 토크나이저와 모델 초기화\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 하이퍼파라미터 가져오기 및 데이터 준비\n",
    "num_epochs, batch_size, learning_rate = get_hyperparameters()\n",
    "train_loader, test_loader = download_and_prepare_data(data_url, tokenizer, batch_size)\n",
    "\n",
    "# 옵티마이저 초기화\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 훈련 루프\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"에포크 {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for input_ids, attention_mask, labels, _, _ in progress_bar:\n",
    "        # 배치를 장치로 이동\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 포워드 패스\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # 백워드 패스 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 메트릭 업데이트\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        progress_bar.set_postfix({\"손실\": total_loss / num_batches})\n",
    "\n",
    "    # 에포크 메트릭 표시\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"에포크 {epoch+1} - 평균 손실: {avg_loss:.4f}\")\n",
    "\n",
    "# 미세 튜닝된 모델 저장\n",
    "model.save_pretrained(\"./finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model\")\n",
    "\n",
    "# 모델 테스트\n",
    "print(\"\\n미세 튜닝된 모델 테스트:\")\n",
    "test_input = \"Who is the President of the United States?\"\n",
    "test_model(\"./finetuned_model\", test_input)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
