{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14de147-340f-46f6-9fde-4865b31f7e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "def set_random_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility across Python, NumPy, and PyTorch.\n",
    "\n",
    "    Parameters:\n",
    "        seed (int): The seed value to use.\n",
    "    \"\"\"\n",
    "    # Set the seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set the seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set the seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Ensure deterministic behavior in cuDNN (may impact performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def prepare_dataset(split=\"train\"):\n",
    "    \"\"\"Load and prepare the GSM8K dataset for training with string prompts.\"\"\"\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split]\n",
    "    formatted_data = []\n",
    "\n",
    "    for example in data:\n",
    "        # Convert list of messages to a single string prompt.\n",
    "        prompt_str = build_prompt([\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": example[\"question\"]}\n",
    "        ])\n",
    "        formatted_example = {\n",
    "            \"prompt\": prompt_str,  # Now a string rather than a list.\n",
    "            \"answer\": extract_answer_from_dataset(example[\"answer\"])\n",
    "        }\n",
    "        formatted_data.append(formatted_example)\n",
    "\n",
    "    return formatted_data\n",
    "\n",
    "def build_prompt(messages):\n",
    "    \"\"\"\n",
    "    Build a single prompt string from a list of messages.\n",
    "    Each message is expected to be a dictionary with 'role' and 'content' keys.\n",
    "    This function concatenates all message contents, preserving the training format.\n",
    "    \"\"\"\n",
    "    return \"\\n\".join([msg[\"content\"].strip() for msg in messages])\n",
    "\n",
    "def extract_answer_from_model_output(text):\n",
    "    \"\"\"\n",
    "    Extracts the value from the last <answer> tag in the text.\n",
    "    Returns None if no valid answer is found.\n",
    "    \"\"\"\n",
    "    # Split on <answer> and take everything after the last occurrence\n",
    "    parts = text.split(\"<answer>\")\n",
    "    if len(parts) < 2:  # No <answer> tag found\n",
    "        return None\n",
    "\n",
    "    last_part = parts[-1]\n",
    "\n",
    "    # Extract content up to </answer>\n",
    "    if \"</answer>\" not in last_part:\n",
    "        return None\n",
    "\n",
    "    answer = last_part.split(\"</answer>\")[0].strip()\n",
    "    return None if answer == \"...\" else answer\n",
    "\n",
    "def extract_answer_from_dataset(text):\n",
    "    \"\"\"\n",
    "    Extracts the answer from the dataset.\n",
    "    The dataset separates the answer using the '####' delimiter.\n",
    "    \"\"\"\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "def _extract_last_number(text):\n",
    "    \"\"\"\n",
    "    Extracts the last number from text if it's properly separated.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to extract a number from.\n",
    "        \n",
    "    Returns:\n",
    "        float or None: The extracted number as a float, or None if no valid number is found.\n",
    "        \n",
    "    Explanation:\n",
    "        1. First removes $ and % signs from the text.\n",
    "        2. Uses regex to find numbers that are:\n",
    "           - Preceded by space, equals sign, or start of string\n",
    "           - Followed by end of string or space\n",
    "        3. Returns the first matching number as a float, or None if no match is found.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    # Remove $ and % signs\n",
    "    text = text.replace('$', '').replace('%', '')\n",
    "\n",
    "    # Look for numbers that are:\n",
    "    # - preceded by space or = or start of string (via \\b or ^)\n",
    "    # - followed by end of string or space\n",
    "    pattern = r'(?:^|\\s|=)\\s*(-?\\d*\\.?\\d+)\\s*$'\n",
    "    match = re.search(pattern, text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "\n",
    "def _extract_single_number(text):\n",
    "    \"\"\"\n",
    "    Extracts a single number from text if exactly one exists.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to extract a number from.\n",
    "        \n",
    "    Returns:\n",
    "        float or None: The extracted number as a float if exactly one number exists,\n",
    "                      otherwise None.\n",
    "        \n",
    "    Explanation:\n",
    "        1. Uses regex to find all numbers in the text.\n",
    "        2. Returns the first number as a float if exactly one number is found.\n",
    "        3. Returns None if zero or multiple numbers are found.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    numbers = re.findall(r'-?\\d*\\.?\\d+', text)\n",
    "    return float(numbers[0]) if len(numbers) == 1 else None\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, eval_examples, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a set of examples and prints detailed results.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model to evaluate.\n",
    "        tokenizer: The tokenizer for encoding inputs and decoding outputs.\n",
    "        eval_examples (list): List of evaluation examples, each containing \"prompt\" and \"answer\".\n",
    "        device: The device (CPU or GPU) to run evaluation on.\n",
    "        \n",
    "    Returns:\n",
    "        float: The accuracy percentage (correct predictions / total examples * 100).\n",
    "        \n",
    "    Explanation:\n",
    "        1. Sets the model to evaluation mode.\n",
    "        2. For each example in the evaluation set:\n",
    "           - Encodes the prompt and generates a response using the model.\n",
    "           - Extracts the predicted answer from the generated response.\n",
    "           - Compares the predicted answer with the expected answer using multiple methods:\n",
    "             a. Exact string matching\n",
    "             b. Single number extraction and comparison\n",
    "             c. Last number extraction and comparison\n",
    "           - Prints detailed information about each example.\n",
    "        3. Calculates and returns the overall accuracy.\n",
    "        4. Returns the model to training mode.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = len(eval_examples)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION ON\", total, \"EXAMPLES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for example in eval_examples:\n",
    "        # Build the full prompt using the same method as training.\n",
    "        full_prompt = example[\"prompt\"]\n",
    "        expected = example[\"answer\"]\n",
    "        \n",
    "        # Tokenize the full prompt and generate a response from the model.\n",
    "        inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            forced_eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract the predicted answer from the model output.\n",
    "        try:\n",
    "            predicted = extract_answer_from_model_output(response)\n",
    "            \n",
    "            # Check correctness in multiple ways\n",
    "            if predicted == expected:  # First try exact match\n",
    "                is_correct = True\n",
    "            else:\n",
    "                # Try single number\n",
    "                pred_num = _extract_single_number(str(predicted))\n",
    "                exp_num = _extract_single_number(str(expected))\n",
    "                if pred_num is not None and exp_num is not None and pred_num == exp_num:\n",
    "                    is_correct = True\n",
    "                else:\n",
    "                    # Try last number\n",
    "                    pred_num = _extract_last_number(str(predicted))\n",
    "                    exp_num = _extract_last_number(str(expected))\n",
    "                    is_correct = (pred_num is not None and exp_num is not None and\n",
    "                                pred_num == exp_num)\n",
    "\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "                \n",
    "            # Print details of the evaluation.\n",
    "            print(\"\\nPrompt:\")\n",
    "            print(full_prompt)\n",
    "            print(\"\\nExpected Answer:\")\n",
    "            print(expected)\n",
    "            print(\"\\nExtracted Answer:\")\n",
    "            print(predicted)\n",
    "            print(\"\\nFull Generated Response:\")\n",
    "            print(response)\n",
    "            print(\"\\nCorrect:\", \"✓\" if is_correct else \"✗\")\n",
    "            print(\"-\"*50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"\\nFailed to parse model output for prompt:\")\n",
    "            print(full_prompt)\n",
    "            print(\"Error:\", e)\n",
    "            print(\"-\"*50)\n",
    "            \n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"\\nAccuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "def correctness_reward(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Assigns a reward based on the correctness of the model's answer.\n",
    "    \n",
    "    Args:\n",
    "        prompts (list[str]): List of prompt texts.\n",
    "        completions (list[list[dict]]): List of completion dictionaries.\n",
    "        answer (list[str]): List of expected answers.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "        \n",
    "    Returns:\n",
    "        list[float]: Reward scores based on answer correctness.\n",
    "        \n",
    "    Explanation:\n",
    "        1. Extracts the text content from each completion.\n",
    "        2. Processes each response to extract the answer portion.\n",
    "        3. Compares extracted answers with expected answers using two methods:\n",
    "           - Exact string matching (2.0 points)\n",
    "           - Numeric equivalence check (1.5 points)\n",
    "        4. Returns a list of reward scores.\n",
    "    \"\"\"\n",
    "    # Extract the content from each completion's first element\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "\n",
    "    # Extract answers from model outputs\n",
    "    extracted = [extract_answer_from_model_output(r) for r in responses]\n",
    "\n",
    "    rewards = []\n",
    "    for r, a in zip(extracted, answer):\n",
    "        if r == a:  # Exact match case\n",
    "            rewards.append(2.0)\n",
    "        else:\n",
    "            # Try numeric equivalence\n",
    "            r_num = _extract_single_number(str(r))\n",
    "            a_num = _extract_single_number(str(a))\n",
    "            if r_num is not None and a_num is not None and r_num == a_num:\n",
    "                rewards.append(1.5)\n",
    "            else:\n",
    "                rewards.append(0.0)\n",
    "\n",
    "    # Log completion lengths\n",
    "    completion_lengths = [len(response.split()) for response in responses]\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Assigns a reward for adhering to the desired XML format.\n",
    "    \n",
    "    Args:\n",
    "        completions (list[list[dict]]): List of completion dictionaries.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "        \n",
    "    Returns:\n",
    "        list[float]: Reward scores based on format compliance.\n",
    "        \n",
    "    Explanation:\n",
    "        1. Extracts the text content from each completion.\n",
    "        2. Assigns points based on the presence of required XML tags:\n",
    "           - 0.2 points for opening <reasoning> tag\n",
    "           - 0.2 points for closing </reasoning> tag\n",
    "           - 0.2 points for opening <answer> tag\n",
    "           - 0.2 points for closing </answer> tag\n",
    "        3. Returns a list of format compliance scores.\n",
    "    \"\"\"\n",
    "    # Extract the content from each completion's first element\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    rewards = []\n",
    "    format_scores = []\n",
    "\n",
    "    for response in responses:\n",
    "        score = 0.0\n",
    "        if \"<reasoning>\" in response: score += 0.2\n",
    "        if \"</reasoning>\" in response: score += 0.2\n",
    "        if \"<answer>\" in response: score += 0.2\n",
    "        if \"</answer>\" in response: score += 0.2\n",
    "        rewards.append(score)\n",
    "        format_scores.append(score)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def combined_reward(prompts, completions, answer):\n",
    "    \"\"\"\n",
    "    Combines correctness and format rewards to provide a comprehensive evaluation.\n",
    "    \n",
    "    Args:\n",
    "        prompts (list[str]): List of prompt texts.\n",
    "        completions (list[list[dict]]): List of completion dictionaries.\n",
    "        answer (list[str]): List of expected answers.\n",
    "        \n",
    "    Returns:\n",
    "        list[float]: Combined rewards for each prompt-completion pair.\n",
    "        \n",
    "    Explanation:\n",
    "        1. Calculates individual reward components:\n",
    "           - Correctness rewards (range: 0.0 to 2.0)\n",
    "           - Format rewards (range: 0.0 to 0.8)\n",
    "        2. Combines the rewards by adding them together.\n",
    "        3. Returns the combined scores with total range of 0.0 to 2.8.\n",
    "    \"\"\"\n",
    "    # Get individual rewards\n",
    "    correctness_scores = correctness_reward(prompts=prompts, completions=completions, answer=answer)\n",
    "    format_scores = format_reward(completions=completions)\n",
    "\n",
    "    # Combine rewards - correctness is weighted more heavily\n",
    "    combined_rewards = []\n",
    "    for c_score, f_score in zip(correctness_scores, format_scores):\n",
    "        # Correctness score range: 0.0 to 2.0\n",
    "        # Format score range: 0.0 to 0.8\n",
    "        # Total range: 0.0 to 2.8\n",
    "        combined_rewards.append(c_score + f_score)\n",
    "\n",
    "    return combined_rewards\n",
    "\n",
    "def selective_log_softmax(logits, input_ids):\n",
    "    \"\"\"\n",
    "    Compute the log probabilities for the tokens specified in input_ids using a selective log-softmax.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): A tensor of shape (batch_size, seq_len, vocab_size) containing raw logits from the model.\n",
    "        input_ids (torch.Tensor): A tensor of shape (batch_size, seq_len) containing the token indices for which we want the log probabilities.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (batch_size, seq_len) where each element is the log probability\n",
    "                      corresponding to the token in input_ids at that position.\n",
    "\n",
    "    Explanation:\n",
    "        1. F.log_softmax is applied along the vocabulary dimension (dim=-1) to convert logits into log probabilities.\n",
    "        2. The tensor input_ids is reshaped (via unsqueeze) to have an extra dimension so that we can use it as indices\n",
    "           in the log_probs tensor.\n",
    "        3. torch.gather collects the log probability at the index specified in input_ids for each position.\n",
    "        4. Finally, squeeze(-1) removes the extra dimension, returning a tensor with the same shape as input_ids.\n",
    "    \"\"\"\n",
    "    # Convert raw logits into log probabilities along the vocabulary axis.\n",
    "    log_probs = F.log_softmax(logits, dim=-1)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "    # Reshape input_ids from (batch_size, seq_len) to (batch_size, seq_len, 1) for gathering.\n",
    "    # Then, gather the log probability for each token in input_ids.\n",
    "    selected_log_probs = log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1))\n",
    "\n",
    "    # Remove the extra last dimension to get back to shape (batch_size, seq_len).\n",
    "    return selected_log_probs.squeeze(-1)\n",
    "\n",
    "def compute_log_probabilities(model, input_ids, attention_mask, logits_to_keep):\n",
    "    \"\"\"\n",
    "    Compute per-token log probabilities for a subset of tokens (typically the completion tokens).\n",
    "\n",
    "    Args:\n",
    "        model: The language model to use.\n",
    "        input_ids (torch.Tensor): Tensor of shape (batch_size, total_seq_len) containing token ids\n",
    "                                  for both prompt and completion.\n",
    "        attention_mask (torch.Tensor): Tensor of shape (batch_size, total_seq_len) indicating which tokens are real (1) or padding (0).\n",
    "        logits_to_keep (int): Number of tokens (from the completion part) for which we need log probabilities.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Log probabilities for the last `logits_to_keep` tokens of each sequence.\n",
    "\n",
    "    Explanation:\n",
    "        1. We call the model with logits_to_keep + 1 so that the model outputs one extra logit than needed.\n",
    "           This is common in next-token prediction setups.\n",
    "        2. We slice off the last logit along the sequence dimension because it does not correspond to any input token.\n",
    "        3. We then restrict both the input_ids and logits to the last logits_to_keep tokens, which should\n",
    "           correspond to the generated completion portion.\n",
    "        4. Finally, we use the selective_log_softmax to compute log probabilities only for those tokens.\n",
    "    \"\"\"\n",
    "    # Run the model forward pass and obtain logits.\n",
    "    logits = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        logits_to_keep=logits_to_keep + 1  # Request one extra logit for proper alignment.\n",
    "    ).logits  # Shape: (batch_size, total_seq_len, vocab_size)\n",
    "\n",
    "    # Remove the last logit as it does not have a corresponding target token.\n",
    "    logits = logits[:, :-1, :]  # New shape: (batch_size, total_seq_len - 1, vocab_size)\n",
    "\n",
    "    # Slice the input_ids to keep only the last logits_to_keep tokens.\n",
    "    # This corresponds to the generated completion tokens.\n",
    "    input_ids = input_ids[:, -logits_to_keep:]  # Shape: (batch_size, logits_to_keep)\n",
    "\n",
    "    # Also slice the logits to keep only those corresponding to the completion tokens.\n",
    "    logits = logits[:, -logits_to_keep:, :]  # Shape: (batch_size, logits_to_keep, vocab_size)\n",
    "\n",
    "    # Compute and return the log probabilities for the selected tokens.\n",
    "    return selective_log_softmax(logits, input_ids)\n",
    "\n",
    "def create_completion_mask(completion_ids, eos_token_id):\n",
    "    \"\"\"\n",
    "    Create a binary mask for the generated completion tokens so that tokens after the first EOS are ignored.\n",
    "\n",
    "    Args:\n",
    "        completion_ids (torch.Tensor): Tensor of shape (batch_size, seq_len) with generated token ids.\n",
    "        eos_token_id (int): The token id representing the end-of-sequence.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A mask tensor of shape (batch_size, seq_len) with 1s for tokens up to and including the first EOS\n",
    "                      and 0s for tokens following the first EOS.\n",
    "\n",
    "    Explanation:\n",
    "        1. First, a boolean mask (is_eos) is created indicating where in the sequence the EOS token appears.\n",
    "        2. An index tensor (eos_idx) is initialized, assuming that no EOS is found (defaulting to the sequence length).\n",
    "        3. For sequences where EOS exists, eos_idx is updated to the position (index) of the first EOS.\n",
    "        4. A sequence index tensor is created that contains indices for each position in the sequence.\n",
    "        5. The final mask is computed by comparing the sequence indices to eos_idx (after adding a dimension).\n",
    "    \"\"\"\n",
    "    # Determine which positions in each sequence equal the EOS token.\n",
    "    is_eos = completion_ids == eos_token_id  # Boolean tensor of shape (batch_size, seq_len)\n",
    "\n",
    "    # Initialize a tensor to store the index of the first EOS for each sequence.\n",
    "    # If no EOS is found, default to the full sequence length (is_eos.size(1)).\n",
    "    eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=completion_ids.device)\n",
    "\n",
    "    # Identify sequences that contain at least one EOS.\n",
    "    mask_exists = is_eos.any(dim=1)\n",
    "    # For sequences with an EOS, update eos_idx to the index of the first occurrence.\n",
    "    eos_idx[mask_exists] = is_eos.int().argmax(dim=1)[mask_exists]\n",
    "\n",
    "    # Create a tensor of indices [0, 1, 2, ..., seq_len-1] and replicate it for each sequence in the batch.\n",
    "    sequence_indices = torch.arange(is_eos.size(1), device=completion_ids.device).expand(is_eos.size(0), -1)\n",
    "\n",
    "    # Build the mask: positions with an index less than or equal to the first EOS index are marked as 1.\n",
    "    completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "    return completion_mask\n",
    "\n",
    "def generate_completions(model, tokenizer, prompts, num_generations=4, max_completion_length=32):\n",
    "    \"\"\"\n",
    "    Generate multiple completions for each prompt and create corresponding attention masks.\n",
    "\n",
    "    Args:\n",
    "        model: The language model used for generation.\n",
    "        tokenizer: The tokenizer to process the prompts and decode the outputs.\n",
    "        prompts (list of str): List of input prompt strings.\n",
    "        num_generations (int): Number of completions to generate per prompt.\n",
    "        max_completion_length (int): Maximum number of new tokens to generate for the completion.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains the following tensors:\n",
    "            - prompt_ids: (batch_size * num_generations, prompt_seq_len)\n",
    "            - prompt_mask: (batch_size * num_generations, prompt_seq_len)\n",
    "            - completion_ids: (batch_size * num_generations, completion_seq_len)\n",
    "            - completion_mask: (batch_size * num_generations, completion_seq_len)\n",
    "\n",
    "    Explanation:\n",
    "        1. The prompts are tokenized and padded (with padding added to the left).\n",
    "        2. Each prompt is repeated num_generations times so that multiple completions are generated per prompt.\n",
    "        3. The model.generate() function is called to generate new tokens.\n",
    "        4. The generated output contains the prompt followed by the completion; we remove the prompt part to get the completions.\n",
    "        5. A mask is created (via create_completion_mask) so that only tokens up to the first EOS are considered.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Tokenize the list of prompts with padding. The padding_side=\"left\" ensures alignment on the right.\n",
    "    tokenizer.padding_side  = \"left\"\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "    prompt_ids = inputs[\"input_ids\"].to(device)      # Shape: (batch_size, prompt_seq_len)\n",
    "    prompt_mask = inputs[\"attention_mask\"].to(device)  # Shape: (batch_size, prompt_seq_len)\n",
    "    prompt_length = prompt_ids.size(1)  # Save the prompt length to later separate prompt from completion.\n",
    "\n",
    "    # Repeat each prompt num_generations times.\n",
    "    prompt_ids = prompt_ids.repeat_interleave(num_generations, dim=0)   # New shape: (batch_size*num_generations, prompt_seq_len)\n",
    "    prompt_mask = prompt_mask.repeat_interleave(num_generations, dim=0) # New shape: (batch_size*num_generations, prompt_seq_len)\n",
    "\n",
    "    # Generate new tokens for each prompt. The output includes the original prompt and the generated tokens.\n",
    "    outputs = model.generate(\n",
    "        prompt_ids,\n",
    "        attention_mask=prompt_mask,\n",
    "        max_new_tokens=max_completion_length,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Remove the prompt portion from the generated output to isolate the completion tokens.\n",
    "    completion_ids = outputs[:, prompt_length:]  # Shape: (batch_size*num_generations, completion_seq_len)\n",
    "\n",
    "    # Create a binary mask that ignores tokens beyond the first EOS token.\n",
    "    completion_mask = create_completion_mask(completion_ids, tokenizer.eos_token_id)\n",
    "\n",
    "    return prompt_ids, prompt_mask, completion_ids, completion_mask\n",
    "\n",
    "def generate_rollout_data(model, ref_model, tokenizer, batch_samples, num_generations, max_completion_length):\n",
    "    \"\"\"\n",
    "    Generate rollouts and compute static log probabilities for both the old policy (current model)\n",
    "    and the reference model. Gradients are disabled so that these remain fixed.\n",
    "\n",
    "    Args:\n",
    "        model: The current model (policy) used to generate rollouts.\n",
    "        ref_model: The static reference model.\n",
    "        tokenizer: The tokenizer.\n",
    "        batch_samples: List of training samples.\n",
    "        num_generations: Number of completions to generate per prompt.\n",
    "        max_completion_length: Maximum completion length.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary with rollout data including both old and reference log probabilities.\n",
    "    \"\"\"\n",
    "    tokenizer.padding_side  = \"left\"\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Extract prompts and answers.\n",
    "    prompts = [sample[\"prompt\"] if isinstance(sample, dict) else sample[0] for sample in batch_samples]\n",
    "    answers = [sample[\"answer\"] if isinstance(sample, dict) else sample[1] for sample in batch_samples]\n",
    "\n",
    "    # Generate completions and associated masks.\n",
    "    # We generate once, and then use the same completions to compute both sets of log probabilities.\n",
    "    with torch.no_grad():\n",
    "        prompt_ids, prompt_mask, completion_ids, completion_mask = generate_completions(\n",
    "            model, tokenizer, prompts, num_generations, max_completion_length\n",
    "        )\n",
    "        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n",
    "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n",
    "        logits_to_keep = completion_ids.size(1)\n",
    "\n",
    "        # Compute old_log_probs from the current model, with gradients disabled.\n",
    "        old_log_probs = compute_log_probabilities(model, input_ids, attention_mask, logits_to_keep)\n",
    "        \n",
    "        # Compute ref_log_probs from the reference model, which remains static.\n",
    "        ref_log_probs = compute_log_probabilities(ref_model, input_ids, attention_mask, logits_to_keep)\n",
    "\n",
    "    formatted_completions = [\n",
    "        [{'content': tokenizer.decode(ids, skip_special_tokens=True)}]\n",
    "        for ids in completion_ids\n",
    "    ]\n",
    "    repeated_prompts = [p for p in prompts for _ in range(num_generations)]\n",
    "    repeated_answers = [a for a in answers for _ in range(num_generations)]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"completion_mask\": completion_mask,\n",
    "        \"old_log_probs\": old_log_probs,   # Static log probs from the current model (old policy)\n",
    "        \"ref_log_probs\": ref_log_probs,     # Static log probs from the reference model\n",
    "        \"formatted_completions\": formatted_completions,\n",
    "        \"repeated_prompts\": repeated_prompts,\n",
    "        \"repeated_answers\": repeated_answers,\n",
    "        \"logits_to_keep\": logits_to_keep,\n",
    "        \"batch_size\": len(prompts),\n",
    "        \"num_generations\": num_generations\n",
    "    }\n",
    "\n",
    "def compute_group_relative_advantages(rewards, num_generations):\n",
    "    \"\"\"\n",
    "    Compute group-relative advantages for each prompt group.\n",
    "    \n",
    "    Args:\n",
    "        rewards (torch.Tensor): Tensor of shape (batch_size * num_generations) containing rewards.\n",
    "        num_generations (int): Number of completions generated per prompt.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of advantages computed relative to the group mean.\n",
    "    \"\"\"\n",
    "    # Reshape rewards to group by prompt\n",
    "    rewards_by_group = rewards.view(-1, num_generations)\n",
    "    \n",
    "    # Compute mean and standard deviation for each prompt group\n",
    "    group_means = rewards_by_group.mean(dim=1)\n",
    "    group_stds = rewards_by_group.std(dim=1)\n",
    "    \n",
    "    # Expand the means and stds to match the original flat rewards tensor shape\n",
    "    expanded_means = group_means.repeat_interleave(num_generations)\n",
    "    expanded_stds = group_stds.repeat_interleave(num_generations)\n",
    "    \n",
    "    # Normalize rewards to get advantages\n",
    "    advantages = (rewards - expanded_means) / (expanded_stds + 1e-4)\n",
    "    \n",
    "    return advantages.unsqueeze(1)  # Add dimension for token-wise operations\n",
    "\n",
    "\n",
    "def maximize_grpo_objective(model, ref_model, rollout_data, tokenizer, reward_function, \n",
    "                          optimizer, beta, epsilon):\n",
    "    \"\"\"\n",
    "    Update the policy model by maximizing the GRPO objective.\n",
    "    \n",
    "    Args:\n",
    "        model: The current policy model.\n",
    "        ref_model: The reference model.\n",
    "        rollout_data: Dictionary containing rollout data.\n",
    "        tokenizer: The tokenizer.\n",
    "        reward_function: Function to compute rewards.\n",
    "        optimizer: The optimizer.\n",
    "        beta (float): KL penalty coefficient.\n",
    "        epsilon (float): Clipping parameter.\n",
    "        \n",
    "    Returns:\n",
    "        float: The loss value.\n",
    "    \"\"\"\n",
    "    # Extract data from rollout\n",
    "    input_ids = rollout_data[\"input_ids\"]\n",
    "    attention_mask = rollout_data[\"attention_mask\"]\n",
    "    completion_mask = rollout_data[\"completion_mask\"]\n",
    "    old_log_probs = rollout_data[\"old_log_probs\"]\n",
    "    ref_log_probs = rollout_data[\"ref_log_probs\"]\n",
    "    logits_to_keep = rollout_data[\"logits_to_keep\"]\n",
    "    \n",
    "    # Compute current log probabilities\n",
    "    current_log_probs = compute_log_probabilities(model, input_ids, attention_mask, logits_to_keep)\n",
    "    \n",
    "    # Compute policy ratio\n",
    "    ratio = torch.exp(current_log_probs - old_log_probs)\n",
    "    \n",
    "    # Get rewards data\n",
    "    formatted_completions = rollout_data[\"formatted_completions\"]\n",
    "    repeated_prompts = rollout_data[\"repeated_prompts\"]\n",
    "    repeated_answers = rollout_data[\"repeated_answers\"]\n",
    "    \n",
    "    # Compute rewards\n",
    "    rewards = torch.tensor(\n",
    "        reward_function(prompts=repeated_prompts, completions=formatted_completions, answer=repeated_answers),\n",
    "        dtype=torch.float32,\n",
    "        device=next(model.parameters()).device\n",
    "    )\n",
    "    avg_reward = rewards.mean().item()\n",
    "    print(f\"Average Reward: {avg_reward:.4f}\")\n",
    "    \n",
    "    # Compute advantages using group-relative normalization\n",
    "    batch_size = rollout_data[\"batch_size\"]\n",
    "    num_generations = rollout_data[\"num_generations\"]\n",
    "    advantages = compute_group_relative_advantages(rewards, num_generations)\n",
    "    \n",
    "    # Compute surrogate loss with clipping\n",
    "    surrogate1 = ratio * advantages\n",
    "    surrogate2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    surrogate_loss = torch.min(surrogate1, surrogate2)\n",
    "    \n",
    "    # Compute KL divergence penalty\n",
    "    kl_div = torch.exp(ref_log_probs - current_log_probs) - (ref_log_probs - current_log_probs) - 1\n",
    "    \n",
    "    # Combine losses\n",
    "    per_token_loss = surrogate_loss - beta * kl_div\n",
    "    loss = -((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n",
    "    \n",
    "    # Optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train_with_grpo(model, tokenizer, train_data, num_iterations=1, \n",
    "                           steps_per_iteration=500, batch_size=4, num_generations=4, \n",
    "                           max_completion_length=128, beta=0.1, learning_rate=5e-6, \n",
    "                           mu=3, epsilon=0.2, reward_function=combined_reward):\n",
    "    \"\"\"\n",
    "    Iterative Group Relative Policy Optimization algorithm.\n",
    "    \n",
    "    Args:\n",
    "        model: The initial policy model to be fine-tuned.\n",
    "        tokenizer: The tokenizer used for encoding prompts and decoding completions.\n",
    "        train_data (list): List of training samples with \"prompt\" and \"answer\" fields.\n",
    "        num_iterations (int): Number of outer iterations (reward model updates).\n",
    "        steps_per_iteration (int): Number of policy update steps per iteration.\n",
    "        batch_size (int): Number of prompt samples per batch.\n",
    "        num_generations (int): Number of completions to generate per prompt.\n",
    "        max_completion_length (int): Maximum token length for completions.\n",
    "        beta (float): KL-divergence penalty coefficient.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        mu (int): Number of GRPO updates per batch of generations.\n",
    "        epsilon (float): Clipping parameter for surrogate objective.\n",
    "        reward_function: Function that evaluates completions and returns rewards.\n",
    "        \n",
    "    Returns:\n",
    "        The fine-tuned policy model.\n",
    "    \"\"\"\n",
    "    # Initialize policy model\n",
    "    policy_model = model\n",
    "    device = next(policy_model.parameters()).device\n",
    "    \n",
    "    # Outer loop for iterations with reward model updates\n",
    "    for iteration in range(1, num_iterations + 1):\n",
    "        print(f\"\\nStarting iteration {iteration}/{num_iterations}\")\n",
    "        \n",
    "        # Create reference model for KL constraint\n",
    "        reference_model = copy.deepcopy(policy_model)\n",
    "        reference_model.eval()\n",
    "        for param in reference_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        reference_model = reference_model.to(device)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        optimizer = torch.optim.Adam(policy_model.parameters(), lr=learning_rate)\n",
    "        policy_model.train()\n",
    "        \n",
    "        # Inner loop for policy updates\n",
    "        for step in range(1, steps_per_iteration + 1):\n",
    "            # Sample batch of prompts\n",
    "            batch_samples = random.sample(train_data, batch_size)\n",
    "            \n",
    "            # Set old policy for this step\n",
    "            with torch.no_grad():\n",
    "                # Generate completions and compute log probs\n",
    "                rollout_data = generate_rollout_data(\n",
    "                    policy_model, reference_model, tokenizer, \n",
    "                    batch_samples, num_generations, max_completion_length\n",
    "                )\n",
    "            \n",
    "            # Multiple GRPO updates per batch of generations\n",
    "            for grpo_iter in range(1, mu + 1):\n",
    "                loss_value = maximize_grpo_objective(\n",
    "                    policy_model, reference_model, rollout_data, tokenizer,\n",
    "                    reward_function, optimizer, beta, epsilon\n",
    "                )\n",
    "                print(f\"Iteration {iteration}/{num_iterations}, Step {step}/{steps_per_iteration}, \"\n",
    "                      f\"GRPO update {grpo_iter}/{mu}, Loss: {loss_value:.4f}\")\n",
    "        \n",
    "        # Optional: Update reward model here if using reward model training\n",
    "        # This is not implemented in the original code but present in the pseudocode\n",
    "        print(f\"Completed iteration {iteration}. Reward model update would happen here.\")\n",
    "    \n",
    "    return policy_model\n",
    "\n",
    "def optimize_model_memory(model):\n",
    "    \"\"\"Apply memory optimizations like proper gradient checkpointing setup\"\"\"\n",
    "    # Ensure model is in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Disable caching for gradient checkpointing\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    # Enable gradient checkpointing\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Enable input gradients properly\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the complete training and evaluation pipeline.\n",
    "\n",
    "    The process consists of:\n",
    "      1. Loading the pre-trained model and tokenizer.\n",
    "      2. Evaluating the initial model performance (before any finetuning).\n",
    "      3. Performing reinforcement learning (GRPO) finetuning.\n",
    "      4. Evaluating the final model after GRPO finetuning.\n",
    "      5. Saving the finetuned model and tokenizer.\n",
    "\n",
    "    Note: Functions such as prepare_dataset, evaluate_model, and reward_function \n",
    "          are assumed to be defined elsewhere.\n",
    "    \"\"\"\n",
    "    # Determine the device (GPU if available, otherwise CPU) from the model's parameters.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Define the model name and output directory.\n",
    "    model_name = \"Qwen/Qwen2.5-0.5B-Instruct\" # The 0.5B model is not smart enough\n",
    "                                              # to generate the <reasoning> and <answer> tags\n",
    "                                              # so several iterations of SFT to teach it these tags\n",
    "                                              # are recommended before RL\n",
    "    output_dir = \"math_solver_model\"\n",
    "\n",
    "    # Load the pre-trained causal language model.\n",
    "    # - torch_dtype specifies the precision (bfloat16 for efficiency on supported hardware).\n",
    "    # - attn_implementation selects an optimized attention mechanism.\n",
    "    # - device_map=\"auto\" automatically distributes the model across available devices.\n",
    "    print(\"Downloading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        #attn_implementation=\"flash_attention_2\",\n",
    "        device_map=None\n",
    "    )\n",
    "    print(\"Downloaded model\")\n",
    "    # Move the model to the determined device.\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Load the tokenizer corresponding to the model.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "    # Set the pad token to be the same as the end-of-sequence token.\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Update the model configuration with the correct token IDs.\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # -------------------------------\n",
    "    # Step 0: INITIAL EVALUATION\n",
    "    # -------------------------------\n",
    "    # Load the complete training dataset using a helper function (assumed defined elsewhere).\n",
    "    all_data = prepare_dataset(\"train\")\n",
    "    # Randomize the order of examples.\n",
    "    random.shuffle(all_data)\n",
    "    # Use a small subset (e.g., 30 examples) for evaluation.\n",
    "    num_eval_examples = 1\n",
    "    eval_data = all_data[:num_eval_examples]\n",
    "\n",
    "    # Evaluate the initial performance of the model before any finetuning.\n",
    "    print(\"\\nInitial model evaluation before GRPO:\")\n",
    "    pre_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device)\n",
    "    print(f\"Pre-GRPO Accuracy: {pre_grpo_accuracy:.2f}%\")\n",
    "\n",
    "    model = optimize_model_memory(model)\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Step 1: RL FINETUNING (GRPO)\n",
    "    # -------------------------------\n",
    "    print(\"\\nStarting RL finetuning using GRPO...\")\n",
    "\n",
    "    # Use the remaining examples (beyond the evaluation subset) for RL finetuning.\n",
    "    train_data = all_data[num_eval_examples:]\n",
    "\n",
    "    # Define RL training configuration.\n",
    "    training_config = {\n",
    "        'num_iterations' : 1,\n",
    "        'steps_per_iteration': 500,                    # Total number of RL training steps.\n",
    "        'batch_size': 4,                     # Number of samples per training step.\n",
    "        'num_generations': 16,                # Number of completions generated per prompt.\n",
    "        'max_completion_length': 500,        # Maximum token length for each generated completion.\n",
    "        'beta': 0.04,                         # KL divergence penalty coefficient.\n",
    "        'learning_rate': 5e-6,                # Learning rate for RL fine-tuning.\n",
    "        'mu': 1,\n",
    "        'epsilon': 0.1,\n",
    "        'reward_function': combined_reward\n",
    "    }\n",
    "    # Fine-tune the model using GRPO RL training.\n",
    "    model = train_with_grpo(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_data=train_data,\n",
    "        **training_config\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # Step 2: FINAL EVALUATION & SAVING\n",
    "    # -------------------------------\n",
    "    print(\"\\nFinal model evaluation after GRPO RL finetuning:\")\n",
    "    # Evaluate the final model performance using the evaluation dataset.\n",
    "    post_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device)\n",
    "    print(f\"Post-GRPO Accuracy: {post_grpo_accuracy:.2f}%\")\n",
    "    print(f\"Total Improvement: {post_grpo_accuracy - pre_grpo_accuracy:.2f}%\")\n",
    "\n",
    "    print(\"\\nSaving GRPO finetuned model...\")\n",
    "    # Save the final finetuned model and tokenizer to disk.\n",
    "    model.save_pretrained(\"grpo_finetuned_model\")\n",
    "    tokenizer.save_pretrained(\"grpo_finetuned_model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
