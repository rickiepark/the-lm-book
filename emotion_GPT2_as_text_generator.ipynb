{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rickiepark/the-lm-book/blob/main/emotion_GPT2_as_text_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f05YFld1tluf"
   },
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <div style=\"background-color: #f4f6f7; padding: 15px; width: 80%;\">\n",
    "        <table style=\"width: 100%\">\n",
    "            <tr>\n",
    "                <td style=\"vertical-align: middle;\">\n",
    "                    <span style=\"font-size: 14px;\">\n",
    "                        A notebook for <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">The Hundred-Page Language Models Book</a> by Andriy Burkov<br><br>\n",
    "                        Code repository: <a href=\"https://github.com/rickiepark/the-lm-book\" target=\"_blank\" rel=\"noopener\">https://github.com/rickiepark/the-lm-book</a>\n",
    "                    </span>\n",
    "                </td>\n",
    "                <td style=\"vertical-align: middle;\">\n",
    "                    <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">\n",
    "                        <img src=\"https://thelmbook.com/img/book.png\" width=\"80px\" alt=\"The Hundred-Page Language Models Book\">\n",
    "                    </a>\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yy0zjL_2ouOU"
   },
   "outputs": [],
   "source": [
    "# 필수 라이브러리 임포트\n",
    "import json             # JSON 데이터 구문 분석을 위한 라이브러리\n",
    "import random          # 시드 설정 및 데이터 셔플링을 위한 라이브러리\n",
    "import gzip            # 데이터셋 압축 해제를 위한 라이브러리\n",
    "import requests        # URL에서 데이터셋 다운로드를 위한 라이브러리\n",
    "import torch           # 메인 파이토치 라이브러리\n",
    "from torch.utils.data import Dataset, DataLoader  # 데이터셋 처리를 위한 라이브러리\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # 허깅 페이스 모델 구성 요소\n",
    "from torch.optim import AdamW    # 훈련을 위한 옵티마이저\n",
    "from tqdm import tqdm   # 진행률 표시줄 유틸리티\n",
    "import re              # 텍스트 정규화를 위한 라이브러리\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    다양한 라이브러리에서 재현성을 위해 난수 시드를 설정합니다.\n",
    "    Args:\n",
    "        seed (int): 난수 생성을 위한 시드 값\n",
    "    \"\"\"\n",
    "    # 파이썬 내장 random 시드 설정\n",
    "    random.seed(seed)\n",
    "    # 파이토치 CPU 난수 시드 설정\n",
    "    torch.manual_seed(seed)\n",
    "    # 사용 가능한 모든 GPU에 대한 시드 설정\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # cuDNN이 결정론적 알고리즘을 사용하도록 요청\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # 일관된 동작을 위해 cuDNN의 자동 튜너 비활성화\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def build_prompt(text):\n",
    "    \"\"\"\n",
    "    감정 분류를 위한 표준화된 프롬프트를 생성합니다.\n",
    "    Args:\n",
    "        text (str): 분류할 입력 텍스트\n",
    "    Returns:\n",
    "        str: 모델을 위한 형식화된 프롬프트\n",
    "    \"\"\"\n",
    "    # 입력 텍스트를 일관된 프롬프트 구조로 형식화\n",
    "    # 명시적인 작업 지시문과 예상 출력 형식 포함\n",
    "    return f\"Predict the emotion for the following text: {text}\\nEmotion:\"\n",
    "\n",
    "def encode_text(tokenizer, text, return_tensor=False):\n",
    "    \"\"\"\n",
    "    제공된 토크나이저를 사용하여 텍스트를 인코딩합니다.\n",
    "    Args:\n",
    "        tokenizer: 허깅 페이스 토크나이저\n",
    "        text (str): 인코딩할 텍스트\n",
    "        return_tensor (bool): 파이토치 텐서를 반환할지 여부\n",
    "    Returns:\n",
    "        토큰 ID의 목록 또는 텐서\n",
    "    \"\"\"\n",
    "    # 텐서 출력이 요청된 경우, 파이토치 텐서로 인코딩\n",
    "    if return_tensor:\n",
    "        return tokenizer.encode(\n",
    "            text, add_special_tokens=False, return_tensors=\"pt\"\n",
    "        )\n",
    "    # 그렇지 않으면 토큰 ID 목록 반환\n",
    "    else:\n",
    "        return tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "def decode_text(tokenizer, token_ids):\n",
    "    \"\"\"\n",
    "    토큰 ID를 텍스트로 다시 디코딩합니다.\n",
    "    Args:\n",
    "        tokenizer: 허깅 페이스 토크나이저\n",
    "        token_ids: 토큰 ID의 목록 또는 텐서\n",
    "    Returns:\n",
    "        str: 디코딩된 텍스트\n",
    "    \"\"\"\n",
    "    # 특수 토큰을 건너뛰고 토큰 ID를 텍스트로 다시 변환\n",
    "    return tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "\n",
    "class PromptCompletionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    프롬프트-완성 쌍을 위한 파이토치 데이터셋.\n",
    "    텍스트 데이터를 모델 준비 형식으로 변환하는 작업을 처리합니다.\n",
    "    Args:\n",
    "        data (list): 프롬프트와 완성을 포함하는 사전 목록\n",
    "        tokenizer: 허깅 페이스 토크나이저\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer):\n",
    "        # 나중에 사용하기 위해 원시 데이터와 토크나이저 저장\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 총 샘플 수 반환\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        단일 훈련 샘플을 반환합니다.\n",
    "        Args:\n",
    "            idx (int): 가져올 샘플의 인덱스\n",
    "        Returns:\n",
    "            dict: input_ids, labels, prompt, expected_completion을 포함\n",
    "        \"\"\"\n",
    "        # 데이터셋에서 특정 샘플 가져오기\n",
    "        item = self.data[idx]\n",
    "        prompt = item[\"prompt\"]\n",
    "        completion = item[\"completion\"]\n",
    "        # 프롬프트와 완성 모두에 대해 텍스트를 토큰 ID로 변환\n",
    "        encoded_prompt = encode_text(self.tokenizer, prompt)\n",
    "        encoded_completion = encode_text(self.tokenizer, completion)\n",
    "        # 시퀀스 끝 토큰 ID 가져오기\n",
    "        eos_token = self.tokenizer.eos_token_id\n",
    "        # EOS 토큰과 함께 프롬프트와 완성 토큰 결합\n",
    "        input_ids = encoded_prompt + encoded_completion + [eos_token]\n",
    "        # 레이블 생성: 프롬프트의 경우 -100(손실에서 무시됨), 학습을 위한 완성 토큰\n",
    "        labels = [-100] * len(encoded_prompt) + encoded_completion + [eos_token]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"prompt\": prompt,\n",
    "            \"expected_completion\": completion\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    샘플 배치를 훈련 준비 형식으로 통합합니다.\n",
    "    패딩과 텐서 변환을 처리합니다.\n",
    "    Args:\n",
    "        batch: 데이터셋의 샘플 목록\n",
    "    Returns:\n",
    "        tuple: (input_ids, attention_mask, labels, prompts, expected_completions)\n",
    "    \"\"\"\n",
    "    # 패딩을 위해 배치에서 가장 긴 시퀀스 찾기\n",
    "    max_length = max(len(item[\"input_ids\"]) for item in batch)\n",
    "    # 패딩 토큰으로 입력 시퀀스를 max_length로 패딩\n",
    "    input_ids = [\n",
    "        item[\"input_ids\"] +\n",
    "        [tokenizer.pad_token_id] * (max_length - len(item[\"input_ids\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "    # -100으로 레이블 시퀀스 패딩(손실 계산에서 무시됨)\n",
    "    labels = [\n",
    "        item[\"labels\"] +\n",
    "        [-100] * (max_length - len(item[\"labels\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "    # 어텐션 마스크 생성: 실제 토큰은 1, 패딩은 0\n",
    "    attention_mask = [\n",
    "        [1] * len(item[\"input_ids\"]) +\n",
    "        [0] * (max_length - len(item[\"input_ids\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "    # 평가를 위해 원본 프롬프트와 완성 유지\n",
    "    prompts = [item[\"prompt\"] for item in batch]\n",
    "    expected_completions = [item[\"expected_completion\"] for item in batch]\n",
    "    # 텍스트를 제외한 모든 항목을 파이토치 텐서로 변환\n",
    "    return (\n",
    "        torch.tensor(input_ids),\n",
    "        torch.tensor(attention_mask),\n",
    "        torch.tensor(labels),\n",
    "        prompts,\n",
    "        expected_completions\n",
    "    )\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    일관된 비교를 위해 텍스트를 정규화합니다.\n",
    "    Args:\n",
    "        text (str): 입력 텍스트\n",
    "    Returns:\n",
    "        str: 정규화된 텍스트\n",
    "    \"\"\"\n",
    "    # 선행/후행 공백 제거 및 소문자로 변환\n",
    "    text = text.strip().lower()\n",
    "    # 여러 공백 문자를 단일 공백으로 대체\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def calculate_accuracy(model, tokenizer, loader):\n",
    "    \"\"\"\n",
    "    데이터셋에서 예측 정확도를 계산합니다.\n",
    "    Args:\n",
    "        model: 미세 튜닝된 모델\n",
    "        tokenizer: 연관된 토크나이저\n",
    "        loader: 평가 샘플을 포함하는 DataLoader\n",
    "    Returns:\n",
    "        float: 정확도 점수\n",
    "    \"\"\"\n",
    "    # 모델을 평가 모드로 설정(드롭아웃 등 비활성화)\n",
    "    model.eval()\n",
    "    # 정확도 계산을 위한 카운터 초기화\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 효율성을 위해 그레이디언트 계산 비활성화\n",
    "    with torch.no_grad():\n",
    "        # 배치 반복\n",
    "        for input_ids, attention_mask, labels, prompts, expected_completions in loader:\n",
    "            # 배치의 각 샘플 처리\n",
    "            for prompt, expected_completion in zip(prompts, expected_completions):\n",
    "                # 이 프롬프트에 대한 모델 예측 생성\n",
    "                generated_text = generate_text(model, tokenizer, prompt)\n",
    "                # 예측과 예상 완성의 정규화된 버전 비교\n",
    "                if normalize_text(generated_text) == normalize_text(expected_completion):\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    # 정확도 계산, 빈 데이터셋 경우 처리\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    # 모델을 훈련 모드로 재설정\n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    주어진 프롬프트에 대한 텍스트 완성을 생성합니다.\n",
    "    Args:\n",
    "        model: 미세 튜닝된 모델\n",
    "        tokenizer: 연관된 토크나이저\n",
    "        prompt (str): 입력 프롬프트\n",
    "        max_new_tokens (int): 생성할 최대 토큰 수\n",
    "    Returns:\n",
    "        str: 생성된 완성\n",
    "    \"\"\"\n",
    "    # 프롬프트 인코딩 및 모델 장치로 이동\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    # 모델을 사용하여 완성 생성\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids[\"input_ids\"],\n",
    "        attention_mask=input_ids[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,        # 더 빠른 생성을 위해 KV 캐시 사용\n",
    "        num_beams=1,           # 탐욕적 디코딩 사용\n",
    "        do_sample=False,       # 샘플링 사용 안 함\n",
    "    )[0]\n",
    "    # 생성된 부분만 추출 및 디코딩(프롬프트 제외)\n",
    "    generated_text = decode_text(tokenizer, output_ids[input_ids[\"input_ids\"].shape[1]:])\n",
    "    return generated_text.strip()\n",
    "\n",
    "def test_model(model_path, test_input):\n",
    "    \"\"\"\n",
    "    저장된 모델을 단일 입력으로 테스트합니다.\n",
    "    Args:\n",
    "        model_path (str): 저장된 모델의 경로\n",
    "        test_input (str): 분류할 텍스트\n",
    "    \"\"\"\n",
    "    # 장치 결정(사용 가능한 경우 GPU, 그렇지 않으면 CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"사용 중인 장치: {device}\")\n",
    "    # 저장된 모델 로드 및 적절한 장치로 이동\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    # 모델에 적절한 패딩 토큰 구성이 있는지 확인\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    # 프롬프트 생성 및 예측 생성\n",
    "    prompt = build_prompt(test_input)\n",
    "    generated_text = generate_text(model, tokenizer, prompt)\n",
    "    # 결과 표시\n",
    "    print(f\"입력: {test_input}\")\n",
    "    print(f\"생성된 감정: {generated_text}\")\n",
    "\n",
    "def download_and_prepare_data(data_url, tokenizer, batch_size, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    훈련을 위해 데이터셋을 다운로드하고 준비합니다.\n",
    "    Args:\n",
    "        data_url (str): 데이터셋의 URL\n",
    "        tokenizer: 텍스트 처리를 위한 토크나이저\n",
    "        batch_size (int): DataLoader의 배치 크기\n",
    "        test_ratio (float): 테스트를 위한 데이터 비율\n",
    "    Returns:\n",
    "        tuple: (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # 압축된 데이터셋 다운로드\n",
    "    response = requests.get(data_url)\n",
    "    # 콘텐츠 압축 해제 및 디코딩\n",
    "    content = gzip.decompress(response.content).decode()\n",
    "    # 각 라인을 JSON으로 구문 분석하고 프롬프트-완성 쌍으로 형식화\n",
    "    dataset = []\n",
    "    for entry in map(json.loads, content.splitlines()):\n",
    "        dataset.append({\n",
    "            \"prompt\": build_prompt(entry['text']),\n",
    "            \"completion\": entry[\"label\"].strip()\n",
    "        })\n",
    "    # 더 나은 분할을 위해 데이터셋 무작위 셔플\n",
    "    random.shuffle(dataset)\n",
    "    # 테스트 비율을 기반으로 분할 인덱스 계산\n",
    "    split_index = int(len(dataset) * (1 - test_ratio))\n",
    "    # 훈련 및 테스트 세트로 분할\n",
    "    train_data = dataset[:split_index]\n",
    "    test_data = dataset[split_index:]\n",
    "    # 데이터셋 객체 생성\n",
    "    train_dataset = PromptCompletionDataset(train_data, tokenizer)\n",
    "    test_dataset = PromptCompletionDataset(test_data, tokenizer)\n",
    "    # 적절한 설정으로 데이터 로더 생성\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,         # 훈련 데이터 셔플\n",
    "        collate_fn=collate_fn  # 패딩을 위한 사용자 정의 통합 함수\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,        # 테스트 데이터는 셔플하지 않음\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_hyperparameters():\n",
    "    \"\"\"\n",
    "    훈련 하이퍼파라미터를 반환합니다.\n",
    "    Returns:\n",
    "        tuple: (num_epochs, batch_size, learning_rate)\n",
    "    \"\"\"\n",
    "    # 학습과 효율성의 균형으로 2 에포크 훈련\n",
    "    num_epochs = 2\n",
    "    # 대부분의 GPU 메모리 크기에서 잘 작동하는 16의 배치 크기\n",
    "    batch_size = 16\n",
    "    # 트랜스포머 미세 튜닝을 위한 표준 학습률\n",
    "    learning_rate = 5e-5\n",
    "    return num_epochs, batch_size, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452,
     "referenced_widgets": [
      "86bf3d889b544d95ba46f5a441fd49a5",
      "a05c2a713a6e44febe10264d046770c3",
      "d85c6bb61dbc4c149b61b5827f574cc1",
      "1156b23044d04f8ea827cc75c51a36f2",
      "f4faf08fe7cb4e7ca5dd990ed05c2f2b",
      "58420029dcf94e7fade72e79db5ca06f",
      "e9a0b2c2aba448588f361d500ddd0759",
      "fdb24bd20f7c4cc699d1117174ad111b",
      "b86b076fc2d24b1a8850ad344edaa12b",
      "4ad251387de14118b019ae22542e243b",
      "f7a6928b1c774b8f8c6de377f7482043",
      "936d7072c21d4eedb33ea229ae9e8b09",
      "8a76fe458e9c4e36af31dcb8c3edf3d8",
      "cad1cba606b34937b825d489be8200b3",
      "0daae774c8844284afe28ca5d86a3af5",
      "90e8da639d9842de93a35fb5890c9fe3",
      "3e535dade4dc46c09f6bfe035eb13f85",
      "d23f6067f1634588a0236a8a08bee3ff",
      "677bba6ba32a4df8b6351cabb8a48afe",
      "1f4bafdc7e5e4dacbee68448e4ef4ca8",
      "0db47e1ce7df467c89c5c83168277393",
      "be19f2730327479886b2f76e6942c72b",
      "4efb449ed36d46d4b5d2e8f4848e88e8",
      "1290e1dc631942bf8dda1748495df69b",
      "3d7d5e3e96104c3d8635f64009ef9e24",
      "c19aa82783494c25a7c9dcd724750808",
      "d794648d79104414800794eb7f1dae9b",
      "e0d8a3bf2ba1461bbb670127fe46189f",
      "a164a8eeb7394c00bb0e4860f5a5c1bc",
      "2d8a5bf73a3e4a3f972be9c276ebd400",
      "1c7d0cb57e384a5ab520235040c9c383",
      "265870b6ba2540d9bc3dc9a815ec7ce2",
      "84e89983f4fc4936813d24913c4bc383",
      "bed89252cae04e709370a91b9d1099f9",
      "8978fa44c9c8407ebd2928946f50746e",
      "1a6ad1d0c5e74c20ade689186208fa0f",
      "990d1dd16a9a4dc788e1861f7569cf03",
      "40f053dcd1da40d38140c0f61605dcea",
      "5a9d1613e8244b78a6c03d1e9305a23e",
      "964240033fba4515b73123d12aaed9ea",
      "5537fe5edbdc41cb9a968b7fed30d8f3",
      "6aa72becaca349bf909699bbb316578a",
      "fd30d7aa02b244c0af18b78e0592241d",
      "5bc575d54ae14699b52bf1cdc7c5f534",
      "593043eaccb54388a52874a45bb56d16",
      "5bdee3ec6c5344af8f05ac88269c0279",
      "13ad9a294b3a463c85f51a09913b9554",
      "86aa7be5115246f1aedcbeccbf0bccd4",
      "67f4cf2ebb0b4f7dbb94159524a53eca",
      "bc64690efcab433586816d8d28070bee",
      "9f2a7cbcb0e14a289b780d7e26e0fff5",
      "13ec308d4bff440ca8113229b7d5997a",
      "fdf912f091b64d49b7026e1d13f4e65b",
      "2637adae682d4f1d8e1c6418f4ae775d",
      "2fdbef46c0b54aebbe7b478f014efb16",
      "8d58eab32db64ba4a48b524f79fbc7d5",
      "e26bc9cc1731484382b8dc7667495340",
      "346c47dfcf494896acb1d5d3b1ac004e",
      "5a34d4baf3ec40daba0ad977adfdd192",
      "1f70c350f61241c8b50a47a06c8dfc7c",
      "034e56b0c35b4699ac6d1ac5133aaf42",
      "6e34baa43652415182e532ce0fbde60b",
      "20b14471d69b467e8e74d2eefe277998",
      "a0e47cd5966f4daba9478eeb44d41f23",
      "2fe525e9d7634dd5b1c4e72d091b8c15",
      "1f73b832fe464b40951d6c9aa11f3b0d",
      "5bad201e10d84cc28b2e12839a13dd59",
      "7a2fa5d875044b35a5e6e4bd38369937",
      "5c715c8b854a46d1845f7e9e166a74fe",
      "8ee3b377abd04da9878ed0df53469de6",
      "550face892124013b3558f87830f218f",
      "bd9249b95d43457aa6c125809642596c",
      "9e4fc49c8dda402ea1bcf36bbf8967cb",
      "1fdcd2e99cf041e98c93a1926123fd06",
      "0c54a5c0674646ef908726503a0a6189",
      "d48c2545fe3e4effb3cf6756b28bfae1",
      "8c976b5a51ee4ec2b7995200a5cc8424"
     ]
    },
    "id": "eeXihzLsThV4",
    "outputId": "b374c91c-84cd-427d-b6e5-6a87bbab9929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 장치: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bf3d889b544d95ba46f5a441fd49a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936d7072c21d4eedb33ea229ae9e8b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efb449ed36d46d4b5d2e8f4848e88e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed89252cae04e709370a91b9d1099f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593043eaccb54388a52874a45bb56d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d58eab32db64ba4a48b524f79fbc7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bad201e10d84cc28b2e12839a13dd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 1/2:   0%|          | 0/1125 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "에포크 1/2: 100%|██████████| 1125/1125 [01:19<00:00, 14.22it/s, 손실=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 1 - 평균 손실: 0.1189, 테스트 정확도: 0.9320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 2/2: 100%|██████████| 1125/1125 [01:18<00:00, 14.41it/s, 손실=0.0591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 2 - 평균 손실: 0.0591, 테스트 정확도: 0.9400\n",
      "훈련 정확도: 0.9456\n",
      "테스트 정확도: 0.9400\n",
      "사용 중인 장치: cuda\n",
      "입력: I'm so happy to be able to finetune an LLM!\n",
      "생성된 감정: joy\n"
     ]
    }
   ],
   "source": [
    "# 재현성을 위해 난수 시드 설정\n",
    "set_seed(42)\n",
    "\n",
    "# 기본 훈련 매개변수 구성\n",
    "data_url = \"https://www.thelmbook.com/data/emotions\"\n",
    "model_name = \"openai-community/gpt2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 장치: {device}\")\n",
    "\n",
    "# 토크나이저 초기화 및 패딩 토큰 구성\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 사전 훈련된 모델 로드 및 적절한 장치로 이동\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 훈련 하이퍼파라미터 가져오기\n",
    "num_epochs, batch_size, learning_rate = get_hyperparameters()\n",
    "\n",
    "# 훈련 데이터 로드 및 준비\n",
    "train_loader, test_loader = download_and_prepare_data(data_url, tokenizer, batch_size)\n",
    "\n",
    "# 학습률로 옵티마이저 초기화\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 훈련 루프\n",
    "for epoch in range(num_epochs):\n",
    "    # 에포크 메트릭 초기화\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # 이 에포크에 대한 진행률 표시줄 생성\n",
    "    progress_bar = tqdm(train_loader, desc=f\"에포크 {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    # 각 배치 처리\n",
    "    for input_ids, attention_mask, labels, _, _ in progress_bar:\n",
    "        # 배치 데이터를 적절한 장치로 이동\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 손실 계산을 포함한 포워드 패스\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # 백워드 패스 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 진행 메트릭 업데이트\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # 현재 손실로 진행률 표시줄 업데이트\n",
    "        progress_bar.set_postfix({\"손실\": total_loss / num_batches})\n",
    "\n",
    "    # 에포크 메트릭 계산 및 표시\n",
    "    avg_loss = total_loss / num_batches\n",
    "    test_acc = calculate_accuracy(model, tokenizer, test_loader)\n",
    "    print(f\"에포크 {epoch+1} - 평균 손실: {avg_loss:.4f}, 테스트 정확도: {test_acc:.4f}\")\n",
    "\n",
    "# 최종 모델 성능 계산\n",
    "train_acc = calculate_accuracy(model, tokenizer, train_loader)\n",
    "print(f\"훈련 정확도: {train_acc:.4f}\")\n",
    "print(f\"테스트 정확도: {test_acc:.4f}\")\n",
    "\n",
    "# 훈련된 모델과 토크나이저 저장\n",
    "model.save_pretrained(\"./finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model\")\n",
    "\n",
    "# 샘플 입력으로 모델 테스트\n",
    "test_input = \"I'm so happy to be able to finetune an LLM!\"\n",
    "test_model(\"./finetuned_model\", test_input)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
