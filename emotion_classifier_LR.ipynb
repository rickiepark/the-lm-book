{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickiepark/the-lm-book/blob/main/emotion_classifier_LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"display: flex; justify-content: center;\">\n",
        "    <div style=\"background-color: #f4f6f7; padding: 15px; width: 80%;\">\n",
        "        <table style=\"width: 100%\">\n",
        "            <tr>\n",
        "                <td style=\"vertical-align: middle;\">\n",
        "                    <span style=\"font-size: 14px;\">\n",
        "                        A notebook for <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">The Hundred-Page Language Models Book</a> by Andriy Burkov<br><br>\n",
        "                        Code repository: <a href=\"https://github.com/rickiepark/the-lm-book\" target=\"_blank\" rel=\"noopener\">https://github.com/rickiepark/the-lm-book</a>\n",
        "                    </span>\n",
        "                </td>\n",
        "                <td style=\"vertical-align: middle;\">\n",
        "                    <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">\n",
        "                        <img src=\"https://thelmbook.com/img/book.png\" width=\"80px\" alt=\"The Hundred-Page Language Models Book\">\n",
        "                    </a>\n",
        "                </td>\n",
        "            </tr>\n",
        "        </table>\n",
        "    </div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "P9WVD-1ZAmYf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy0zjL_2ouOU",
        "outputId": "d140cee3-baa7-47f2-c171-f2934e1a7fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 샘플 수: 18000\n",
            "테스트 샘플 수: 2000\n",
            "\n",
            "훈련 정확도: 0.9854\n",
            "테스트 정확도: 0.8880\n",
            "\n",
            "--- 더 나은 하이퍼파라미터 ---\n",
            "훈련 정확도: 0.9963\n",
            "테스트 정확도: 0.8890\n"
          ]
        }
      ],
      "source": [
        "# 필수 라이브러리 임포트\n",
        "import gzip             # gzip 압축 데이터 파일 해제를 위한 라이브러리\n",
        "import json             # JSON 형식 데이터 구문 분석을 위한 라이브러리\n",
        "import random           # 데이터셋 셔플링 및 시드 설정을 위한 라이브러리\n",
        "import requests         # URL에서 데이터셋 다운로드를 위한 라이브러리\n",
        "from sklearn.feature_extraction.text import CountVectorizer # 텍스트 벡터화 유틸리티\n",
        "from sklearn.linear_model import LogisticRegression         # 로지스틱 회귀 모델\n",
        "from sklearn.metrics import accuracy_score                  # 모델 평가를 위한 라이브러리\n",
        "\n",
        "# ----------------------------\n",
        "# 유틸리티 함수\n",
        "# ----------------------------\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    재현성을 위해 난수 시드를 설정합니다.\n",
        "    Args:\n",
        "        seed (int): 난수 생성을 위한 시드 값\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "\n",
        "def download_and_split_data(data_url, test_ratio=0.1):\n",
        "    \"\"\"\n",
        "    URL에서 감정 분류 데이터셋을 다운로드하고 훈련/테스트 세트로 분할합니다.\n",
        "    원시 데이터의 압축 해제 및 JSON 구문 분석을 처리합니다.\n",
        "    Args:\n",
        "        data_url (str): gzip 압축된 JSON 데이터셋의 URL\n",
        "        test_ratio (float): 테스트에 사용할 데이터 비율 (기본값: 0.1)\n",
        "    Returns:\n",
        "        tuple: (X_train, y_train, X_test, y_test) 포함:\n",
        "            - X_train, X_test: 훈련 및 테스트를 위한 텍스트 샘플 목록\n",
        "            - y_train, y_test: 해당 감정 레이블 목록\n",
        "    \"\"\"\n",
        "    # 데이터셋 다운로드 및 압축 해제\n",
        "    response = requests.get(data_url)\n",
        "    content = gzip.decompress(response.content).decode()\n",
        "    # JSON 라인을 딕셔너리의 리스트로 파싱합니다.\n",
        "    dataset = [json.loads(line) for line in content.splitlines()]\n",
        "    # 무작위 분할을 위해 데이터셋 셔플링\n",
        "    random.shuffle(dataset)\n",
        "    # 훈련 및 테스트 세트로 분할\n",
        "    split_index = int(len(dataset) * (1 - test_ratio))\n",
        "    train, test = dataset[:split_index], dataset[split_index:]\n",
        "    # 텍스트와 레이블 분리\n",
        "    X_train = [item[\"text\"] for item in train]\n",
        "    y_train = [item[\"label\"] for item in train]\n",
        "    X_test = [item[\"text\"] for item in test]\n",
        "    y_test = [item[\"label\"] for item in test]\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# ----------------------------\n",
        "# 메인 실행\n",
        "# ----------------------------\n",
        "# 재현성을 위해 난수 시드 설정\n",
        "set_seed(42)\n",
        "\n",
        "# 데이터셋 다운로드 및 준비\n",
        "data_url = \"https://www.thelmbook.com/data/emotions\"\n",
        "X_train_text, y_train, X_test_text, y_test = download_and_split_data(\n",
        "    data_url, test_ratio=0.1\n",
        ")\n",
        "print(\"훈련 샘플 수:\", len(X_train_text))\n",
        "print(\"테스트 샘플 수:\", len(X_test_text))\n",
        "\n",
        "# ----------------------------\n",
        "# 기준 모델\n",
        "# ----------------------------\n",
        "# 기본 매개변수로 텍스트 벡터라이저 초기화\n",
        "# max_features=10_000: 어휘사전을 상위 10k개의 가장 빈번한 단어로 제한\n",
        "# binary=True: 개수를 이진 표시기(0/1)로 변환\n",
        "vectorizer = CountVectorizer(max_features=10_000, binary=True)\n",
        "\n",
        "# 텍스트 데이터를 수치 특성으로 변환\n",
        "X_train = vectorizer.fit_transform(X_train_text)\n",
        "X_test = vectorizer.transform(X_test_text)\n",
        "\n",
        "# 로지스틱 회귀 모델 초기화 및 훈련\n",
        "model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 훈련 및 테스트 세트에서 예측 수행\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# 정확도 메트릭 계산 및 표시\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"\\n훈련 정확도: {train_accuracy:.4f}\")\n",
        "print(f\"테스트 정확도: {test_accuracy:.4f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 개선된 모델\n",
        "# ----------------------------\n",
        "print(\"\\n--- 더 나은 하이퍼파라미터 ---\")\n",
        "# 개선된 매개변수로 벡터라이저 초기화\n",
        "# max_features=20000: 증가된 어휘 크기\n",
        "# ngram_range=(1, 2): 유니그램과 바이그램 모두 포함\n",
        "vectorizer = CountVectorizer(max_features=20000, ngram_range=(1, 2))\n",
        "\n",
        "# 새 벡터라이저로 텍스트 데이터 변환\n",
        "X_train = vectorizer.fit_transform(X_train_text)\n",
        "X_test = vectorizer.transform(X_test_text)\n",
        "\n",
        "# 동일한 매개변수로 모델 훈련 및 평가\n",
        "model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"훈련 정확도: {train_accuracy:.4f}\")\n",
        "print(f\"테스트 정확도: {test_accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}